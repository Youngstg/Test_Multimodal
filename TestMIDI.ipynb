{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOV/zia9qPoppYMnq4/LFA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youngstg/Test_Multimodal/blob/main/TestMIDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MULTIMODAL MUSIC EMOTION CLASSIFICATION\n",
        "# Part 2: Orpheus MIDI-based Emotion Classification dengan 5-Fold Cross Validation\n",
        "\n",
        "# Dataset: MIREX Emotion Dataset dari Kaggle\n",
        "# Modalitas: MIDI (Musical Instrument Digital Interface)\n",
        "# Model: Orpheus (MIDI Encoder)\n",
        "\n",
        "# Paper: \"Orpheus: A Lightweight Transformer for Music Understanding\""
      ],
      "metadata": {
        "id": "IdgN7IFeDjqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. INSTALASI DAN IMPORT LIBRARY"
      ],
      "metadata": {
        "id": "aDSQ9sQHDu9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing required packages...\")\n",
        "!pip install -q kagglehub\n",
        "!pip install -q transformers torch torchvision torchaudio\n",
        "!pip install -q scikit-learn pandas numpy\n",
        "!pip install -q pretty_midi mido  # MIDI processing\n",
        "!pip install -q music21  # Advanced MIDI analysis\n",
        "\n",
        "print(\"✓ Installation complete!\")\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pretty_midi\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'\\n✓ Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTbcVaWDDsjI",
        "outputId": "df387d42-39a1-479b-e98f-9383beda4edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "✓ Installation complete!\n",
            "\n",
            "✓ Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. DOWNLOAD DAN LOAD DATASET"
      ],
      "metadata": {
        "id": "I9-sjr3dDxXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DOWNLOADING MIREX DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"imsparsh/multimodal-mirex-emotion-dataset\")\n",
        "print(f\"✓ Path to dataset files: {path}\")\n",
        "\n",
        "# Explore MIDI directory\n",
        "midi_dir = os.path.join(path, 'dataset', 'MIDI')\n",
        "print(f\"\\n✓ MIDI directory: {midi_dir}\")\n",
        "print(f\"✓ MIDI directory exists: {os.path.exists(midi_dir)}\")\n",
        "\n",
        "if os.path.exists(midi_dir):\n",
        "    midi_files = [f for f in os.listdir(midi_dir) if f.endswith('.mid') or f.endswith('.midi')]\n",
        "    print(f\"✓ Found {len(midi_files)} MIDI files\")\n",
        "    print(f\"  Sample files: {midi_files[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2W1JXzSDx8Y",
        "outputId": "5da73ef4-e591-4ebc-97d1-753b4c48d3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "DOWNLOADING MIREX DATASET\n",
            "================================================================================\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/imsparsh/multimodal-mirex-emotion-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 305M/305M [00:02<00:00, 139MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Path to dataset files: /root/.cache/kagglehub/datasets/imsparsh/multimodal-mirex-emotion-dataset/versions/1\n",
            "\n",
            "✓ MIDI directory: /root/.cache/kagglehub/datasets/imsparsh/multimodal-mirex-emotion-dataset/versions/1/dataset/MIDI\n",
            "✓ MIDI directory exists: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. LOAD CLUSTER LABELS (SAME AS LYRICS)"
      ],
      "metadata": {
        "id": "KnThEs6HD2RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cluster_labels(dataset_path):\n",
        "    \"\"\"\n",
        "    Load cluster labels from clusters.txt\n",
        "    \"\"\"\n",
        "    clusters_path = os.path.join(dataset_path, 'dataset', 'clusters.txt')\n",
        "    cluster_labels = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LOADING CLUSTER LABELS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if os.path.exists(clusters_path):\n",
        "        with open(clusters_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "            cluster_labels = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        print(f\"✓ Loaded {len(cluster_labels)} cluster labels\")\n",
        "        print(f\"✓ Unique clusters: {unique_clusters}\")\n",
        "        print(f\"✓ Number of clusters: {len(unique_clusters)}\")\n",
        "\n",
        "        # Show distribution\n",
        "        from collections import Counter\n",
        "        cluster_counts = Counter(cluster_labels)\n",
        "        print(f\"\\nCluster distribution:\")\n",
        "        for cluster, count in sorted(cluster_counts.items()):\n",
        "            print(f\"  {cluster}: {count} songs\")\n",
        "    else:\n",
        "        print(\"❌ clusters.txt not found!\")\n",
        "        return []\n",
        "\n",
        "    return cluster_labels\n",
        "\n",
        "cluster_labels = load_cluster_labels(path)\n",
        "\n",
        "# Create song_id to cluster mapping\n",
        "song_cluster_map = {}\n",
        "for idx in range(len(cluster_labels)):\n",
        "    song_id_0 = str(idx).zfill(3)\n",
        "    song_id_1 = str(idx + 1).zfill(3)\n",
        "    song_cluster_map[song_id_0] = cluster_labels[idx]\n",
        "    song_cluster_map[song_id_1] = cluster_labels[idx]\n",
        "\n",
        "print(f\"\\n✓ Created mappings for {len(song_cluster_map)} song IDs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbyd8SR8D3oy",
        "outputId": "e25697b0-8b1d-4892-fa73-641781d58684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING CLUSTER LABELS\n",
            "================================================================================\n",
            "✓ Loaded 903 cluster labels\n",
            "✓ Unique clusters: ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5']\n",
            "✓ Number of clusters: 5\n",
            "\n",
            "Cluster distribution:\n",
            "  Cluster 1: 170 songs\n",
            "  Cluster 2: 164 songs\n",
            "  Cluster 3: 215 songs\n",
            "  Cluster 4: 191 songs\n",
            "  Cluster 5: 163 songs\n",
            "\n",
            "✓ Created mappings for 904 song IDs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. MIDI PREPROCESSING & FEATURE EXTRACTION"
      ],
      "metadata": {
        "id": "7wdeQ0TUD59E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_midi_features(midi_path, max_length=512):\n",
        "    \"\"\"\n",
        "    Extract features from MIDI file using pretty_midi\n",
        "\n",
        "    Features extracted:\n",
        "    - Note sequences (pitch, velocity, duration)\n",
        "    - Tempo\n",
        "    - Time signature\n",
        "    - Key signature (if available)\n",
        "    - Instrument information\n",
        "    \"\"\"\n",
        "    try:\n",
        "        midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Extract note sequences\n",
        "        notes = []\n",
        "        for instrument in midi_data.instruments:\n",
        "            if not instrument.is_drum:  # Skip drum tracks\n",
        "                for note in instrument.notes:\n",
        "                    notes.append({\n",
        "                        'pitch': note.pitch,\n",
        "                        'velocity': note.velocity,\n",
        "                        'start': note.start,\n",
        "                        'end': note.end,\n",
        "                        'duration': note.end - note.start\n",
        "                    })\n",
        "\n",
        "        # Sort by start time\n",
        "        notes = sorted(notes, key=lambda x: x['start'])\n",
        "\n",
        "        # Limit to max_length notes\n",
        "        notes = notes[:max_length]\n",
        "\n",
        "        # Convert to sequences\n",
        "        pitch_seq = [n['pitch'] for n in notes]\n",
        "        velocity_seq = [n['velocity'] for n in notes]\n",
        "        duration_seq = [n['duration'] for n in notes]\n",
        "\n",
        "        # Pad sequences\n",
        "        while len(pitch_seq) < max_length:\n",
        "            pitch_seq.append(0)\n",
        "            velocity_seq.append(0)\n",
        "            duration_seq.append(0)\n",
        "\n",
        "        # Extract tempo (average)\n",
        "        tempo_changes = midi_data.get_tempo_changes()\n",
        "        avg_tempo = np.mean(tempo_changes[1]) if len(tempo_changes[1]) > 0 else 120.0\n",
        "\n",
        "        # Extract time signature (first occurrence)\n",
        "        time_sigs = midi_data.time_signature_changes\n",
        "        if len(time_sigs) > 0:\n",
        "            numerator = time_sigs[0].numerator\n",
        "            denominator = time_sigs[0].denominator\n",
        "        else:\n",
        "            numerator = 4\n",
        "            denominator = 4\n",
        "\n",
        "        features = {\n",
        "            'pitch_sequence': np.array(pitch_seq[:max_length]),\n",
        "            'velocity_sequence': np.array(velocity_seq[:max_length]),\n",
        "            'duration_sequence': np.array(duration_seq[:max_length]),\n",
        "            'avg_tempo': avg_tempo,\n",
        "            'time_sig_numerator': numerator,\n",
        "            'time_sig_denominator': denominator,\n",
        "            'num_notes': min(len(notes), max_length)\n",
        "        }\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {midi_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def quantize_midi_features(features):\n",
        "    \"\"\"\n",
        "    Quantize MIDI features for tokenization\n",
        "    \"\"\"\n",
        "    # Normalize pitch (0-127 -> bins)\n",
        "    pitch_bins = np.clip(features['pitch_sequence'] // 12, 0, 10)  # Octave-based binning\n",
        "\n",
        "    # Normalize velocity (0-127 -> 4 bins: pp, p, mf, f, ff)\n",
        "    velocity_bins = np.clip(features['velocity_sequence'] // 32, 0, 3)\n",
        "\n",
        "    # Normalize duration (quantize to musical note values)\n",
        "    duration_bins = np.clip((features['duration_sequence'] * 4).astype(int), 0, 15)\n",
        "\n",
        "    return {\n",
        "        'pitch_bins': pitch_bins,\n",
        "        'velocity_bins': velocity_bins,\n",
        "        'duration_bins': duration_bins,\n",
        "        'tempo': features['avg_tempo'],\n",
        "        'time_sig': (features['time_sig_numerator'], features['time_sig_denominator']),\n",
        "        'num_notes': features['num_notes']\n",
        "    }\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING MIDI DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# First, let's explore the actual MIDI directory structure\n",
        "print(\"\\n--- Exploring MIDI directory structure ---\")\n",
        "dataset_dir = os.path.join(path, 'dataset')\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Exists: {os.path.exists(dataset_dir)}\")\n",
        "\n",
        "# List all subdirectories\n",
        "if os.path.exists(dataset_dir):\n",
        "    subdirs = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
        "    print(f\"\\nSubdirectories in dataset/: {subdirs}\")\n",
        "\n",
        "    # Check each directory for MIDI files\n",
        "    for subdir in subdirs:\n",
        "        subdir_path = os.path.join(dataset_dir, subdir)\n",
        "        files = os.listdir(subdir_path)\n",
        "        midi_files_in_dir = [f for f in files if f.endswith('.mid') or f.endswith('.midi') or f.endswith('.MID')]\n",
        "        if len(midi_files_in_dir) > 0:\n",
        "            print(f\"\\n  {subdir}/: {len(midi_files_in_dir)} MIDI files\")\n",
        "            print(f\"    Sample: {midi_files_in_dir[:3]}\")\n",
        "\n",
        "# Try multiple possible MIDI directory names\n",
        "possible_midi_dirs = [\n",
        "    os.path.join(path, 'dataset', 'MIDIs'),  # ADD THIS - with 's'\n",
        "    os.path.join(path, 'dataset', 'MIDI'),\n",
        "    os.path.join(path, 'dataset', 'Midi'),\n",
        "    os.path.join(path, 'dataset', 'midi'),\n",
        "    os.path.join(path, 'MIDIs'),\n",
        "    os.path.join(path, 'MIDI'),\n",
        "    os.path.join(path, 'Midi'),\n",
        "    os.path.join(path, 'midi'),\n",
        "]\n",
        "\n",
        "midi_dir = None\n",
        "for possible_dir in possible_midi_dirs:\n",
        "    if os.path.exists(possible_dir):\n",
        "        files = os.listdir(possible_dir)\n",
        "        midi_files = [f for f in files if f.endswith('.mid') or f.endswith('.midi') or f.endswith('.MID')]\n",
        "        if len(midi_files) > 0:\n",
        "            midi_dir = possible_dir\n",
        "            print(f\"\\n✓ Found MIDI directory: {midi_dir}\")\n",
        "            print(f\"✓ Contains {len(midi_files)} MIDI files\")\n",
        "            break\n",
        "\n",
        "if midi_dir is None:\n",
        "    print(\"\\n❌ ERROR: No MIDI directory found!\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"1. Does the dataset contain MIDI files?\")\n",
        "    print(\"2. What is the exact directory structure?\")\n",
        "    raise ValueError(\"MIDI directory not found in dataset\")\n",
        "\n",
        "# Load MIDI files and create dataset\n",
        "midi_data_list = []\n",
        "\n",
        "midi_files = [f for f in os.listdir(midi_dir) if f.endswith('.mid') or f.endswith('.midi') or f.endswith('.MID')]\n",
        "print(f\"\\nProcessing {len(midi_files)} MIDI files...\")\n",
        "\n",
        "matched = 0\n",
        "failed = 0\n",
        "no_cluster = 0\n",
        "\n",
        "for idx, midi_file in enumerate(midi_files):\n",
        "    # Show progress every 50 files\n",
        "    if idx % 50 == 0 and idx > 0:\n",
        "        print(f\"  Progress: {idx}/{len(midi_files)} files processed...\")\n",
        "\n",
        "    # Extract song ID - try different patterns\n",
        "    song_id = midi_file.replace('.mid', '').replace('.midi', '').replace('.MID', '')\n",
        "\n",
        "    # Try to clean song_id (remove extensions and extra chars)\n",
        "    song_id_clean = ''.join(filter(str.isdigit, song_id))\n",
        "    if song_id_clean:\n",
        "        # Pad with zeros if needed\n",
        "        song_id = song_id_clean.zfill(3)\n",
        "\n",
        "    # Check if we have cluster label\n",
        "    if song_id not in song_cluster_map:\n",
        "        no_cluster += 1\n",
        "        if no_cluster <= 3:\n",
        "            print(f\"  ⚠️ No cluster for: {midi_file} (extracted ID: {song_id})\")\n",
        "        continue\n",
        "\n",
        "    # Extract features\n",
        "    midi_path = os.path.join(midi_dir, midi_file)\n",
        "    features = extract_midi_features(midi_path)\n",
        "\n",
        "    if features is not None and features['num_notes'] > 0:\n",
        "        # Quantize\n",
        "        quantized = quantize_midi_features(features)\n",
        "\n",
        "        midi_data_list.append({\n",
        "            'song_id': song_id,\n",
        "            'features': quantized,\n",
        "            'cluster': song_cluster_map[song_id]\n",
        "        })\n",
        "        matched += 1\n",
        "\n",
        "        if matched <= 3:\n",
        "            print(f\"  ✓ Loaded: {midi_file} → ID: {song_id} → {song_cluster_map[song_id]} ({quantized['num_notes']} notes)\")\n",
        "    else:\n",
        "        failed += 1\n",
        "        if failed <= 3:\n",
        "            print(f\"  ❌ Failed to extract features: {midi_file}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"MIDI LOADING SUMMARY:\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"✓ Successfully loaded: {matched} MIDI files\")\n",
        "print(f\"⚠️ No cluster mapping: {no_cluster} files\")\n",
        "print(f\"❌ Failed to process: {failed} files\")\n",
        "print(f\"Total processed: {len(midi_files)} files\")\n",
        "\n",
        "# Create DataFrame\n",
        "if len(midi_data_list) > 0:\n",
        "    df = pd.DataFrame(midi_data_list)\n",
        "    print(f\"\\n✓ Dataset shape: {df.shape}\")\n",
        "    print(f\"✓ Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    print(f\"\\nCluster distribution:\")\n",
        "    print(df['cluster'].value_counts())\n",
        "else:\n",
        "    print(\"\\n❌ ERROR: No MIDI data successfully loaded!\")\n",
        "    print(\"\\nDebugging info:\")\n",
        "    print(f\"  Total MIDI files found: {len(midi_files)}\")\n",
        "    print(f\"  Files with no cluster: {no_cluster}\")\n",
        "    print(f\"  Files failed to process: {failed}\")\n",
        "    print(f\"  Sample MIDI filenames: {midi_files[:5]}\")\n",
        "    print(f\"  Sample song_cluster_map keys: {list(song_cluster_map.keys())[:10]}\")\n",
        "\n",
        "    raise ValueError(\"No MIDI data loaded! Check filename format and cluster mapping.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE9924YbD8PC",
        "outputId": "3c129d52-0df5-48d8-8003-e3b707ff4b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING MIDI DATA\n",
            "================================================================================\n",
            "\n",
            "--- Exploring MIDI directory structure ---\n",
            "Dataset directory: /root/.cache/kagglehub/datasets/imsparsh/multimodal-mirex-emotion-dataset/versions/1/dataset\n",
            "Exists: True\n",
            "\n",
            "Subdirectories in dataset/: ['MIDIs', 'Audio', 'Lyrics']\n",
            "\n",
            "  MIDIs/: 196 MIDI files\n",
            "    Sample: ['037.mid', '097.mid', '552.mid']\n",
            "\n",
            "✓ Found MIDI directory: /root/.cache/kagglehub/datasets/imsparsh/multimodal-mirex-emotion-dataset/versions/1/dataset/MIDIs\n",
            "✓ Contains 196 MIDI files\n",
            "\n",
            "Processing 196 MIDI files...\n",
            "  ✓ Loaded: 037.mid → ID: 037 → Cluster 1 (512 notes)\n",
            "Error processing /root/.cache/kagglehub/datasets/imsparsh/multimodal-mirex-emotion-dataset/versions/1/dataset/MIDIs/097.mid: data byte must be in range 0..127\n",
            "  ❌ Failed to extract features: 097.mid\n",
            "  ✓ Loaded: 552.mid → ID: 552 → Cluster 4 (512 notes)\n",
            "  ✓ Loaded: 108.mid → ID: 108 → Cluster 1 (512 notes)\n",
            "  Progress: 50/196 files processed...\n",
            "  Progress: 100/196 files processed...\n",
            "  Progress: 150/196 files processed...\n",
            "Error processing /root/.cache/kagglehub/datasets/imsparsh/multimodal-mirex-emotion-dataset/versions/1/dataset/MIDIs/009.mid: data byte must be in range 0..127\n",
            "  ❌ Failed to extract features: 009.mid\n",
            "\n",
            "================================================================================\n",
            "MIDI LOADING SUMMARY:\n",
            "================================================================================\n",
            "✓ Successfully loaded: 194 MIDI files\n",
            "⚠️ No cluster mapping: 0 files\n",
            "❌ Failed to process: 2 files\n",
            "Total processed: 196 files\n",
            "\n",
            "✓ Dataset shape: (194, 3)\n",
            "✓ Columns: ['song_id', 'features', 'cluster']\n",
            "\n",
            "Cluster distribution:\n",
            "cluster\n",
            "Cluster 3    48\n",
            "Cluster 2    44\n",
            "Cluster 1    43\n",
            "Cluster 4    33\n",
            "Cluster 5    26\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. ORPHEUS-INSPIRED MIDI ENCODER"
      ],
      "metadata": {
        "id": "frfpK0SuEG5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OrpheusMIDIEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    SIMPLIFIED Orpheus-inspired MIDI encoder for small datasets\n",
        "    Drastically reduced model capacity to prevent overfitting\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=128, d_model=64, nhead=2, num_layers=1, dropout=0.5):  # MUCH SMALLER!\n",
        "        super(OrpheusMIDIEncoder, self).__init__()\n",
        "\n",
        "        # Smaller embedding layers\n",
        "        self.pitch_embedding = nn.Embedding(vocab_size, d_model // 4)\n",
        "        self.velocity_embedding = nn.Embedding(32, d_model // 4)\n",
        "        self.duration_embedding = nn.Embedding(64, d_model // 4)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = nn.Embedding(512, d_model // 4)\n",
        "\n",
        "        # Projection to d_model\n",
        "        self.input_projection = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # SINGLE Transformer layer only (was 4!)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model * 2,  # Smaller feedforward\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output projection\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, pitch, velocity, duration, attention_mask=None):\n",
        "        batch_size, seq_len = pitch.shape\n",
        "\n",
        "        # Embed different attributes\n",
        "        pitch_emb = self.pitch_embedding(pitch)\n",
        "        velocity_emb = self.velocity_embedding(velocity)\n",
        "        duration_emb = self.duration_embedding(duration)\n",
        "\n",
        "        # Positional encoding\n",
        "        positions = torch.arange(seq_len, device=pitch.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        pos_emb = self.pos_encoder(positions)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        x = torch.cat([pitch_emb, velocity_emb, duration_emb, pos_emb], dim=-1)\n",
        "\n",
        "        # Project to d_model\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Create attention mask for padding\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = ~attention_mask.bool()\n",
        "\n",
        "        # Transformer encoding\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=attention_mask)\n",
        "\n",
        "        # Global average pooling\n",
        "        if attention_mask is not None:\n",
        "            mask_expanded = (~attention_mask).unsqueeze(-1).float()\n",
        "            x = (x * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1)\n",
        "        else:\n",
        "            x = x.mean(dim=1)\n",
        "\n",
        "        # Layer norm and dropout\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class OrpheusEmotionClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Extremely simplified classifier for small dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, d_model=64, nhead=2, num_layers=1, dropout=0.7):\n",
        "        super(OrpheusEmotionClassifier, self).__init__()\n",
        "\n",
        "        # Simplified encoder\n",
        "        self.encoder = OrpheusMIDIEncoder(\n",
        "            vocab_size=128,\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout * 0.7\n",
        "        )\n",
        "\n",
        "        # DIRECT classification (no hidden layer!)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, pitch, velocity, duration, attention_mask=None):\n",
        "        # Encode MIDI\n",
        "        embedding = self.encoder(pitch, velocity, duration, attention_mask)\n",
        "\n",
        "        # Direct classify\n",
        "        x = self.dropout(embedding)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        return logits, embedding"
      ],
      "metadata": {
        "id": "e_KR9mNxEDiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. DATASET CLASS"
      ],
      "metadata": {
        "id": "9h0H5cwuEJtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MIDIDataset(Dataset):\n",
        "    def __init__(self, data, max_length=512):\n",
        "        self.data = data\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        features = item['features']\n",
        "\n",
        "        # Get sequences\n",
        "        pitch = torch.tensor(features['pitch_bins'], dtype=torch.long)\n",
        "        velocity = torch.tensor(features['velocity_bins'], dtype=torch.long)\n",
        "        duration = torch.tensor(features['duration_bins'], dtype=torch.long)\n",
        "\n",
        "        # Create attention mask (1 for real notes, 0 for padding)\n",
        "        attention_mask = torch.zeros(self.max_length, dtype=torch.float)\n",
        "        num_notes = min(features['num_notes'], self.max_length)\n",
        "        attention_mask[:num_notes] = 1.0\n",
        "\n",
        "        return {\n",
        "            'pitch': pitch,\n",
        "            'velocity': velocity,\n",
        "            'duration': duration,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': item['label']\n",
        "        }"
      ],
      "metadata": {
        "id": "S3sMaabOELwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. LABEL ENCODING"
      ],
      "metadata": {
        "id": "3u_aLmtwEOdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENCODING LABELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['cluster'])\n",
        "\n",
        "print(f\"✓ Cluster classes: {label_encoder.classes_}\")\n",
        "print(f\"✓ Number of clusters: {len(label_encoder.classes_)}\")\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "for cluster, count in df['cluster'].value_counts().items():\n",
        "    encoded = df[df['cluster'] == cluster]['label'].iloc[0]\n",
        "    print(f\"  {encoded}: {cluster} - {count} samples\")\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Calculate class weights\n",
        "y_labels = df['label'].values\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_labels), y=y_labels)\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "print(f\"\\n✓ Class weights: {class_weights.cpu().numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVCsWgr6EUyn",
        "outputId": "16d9d844-01d8-4c54-b0e8-4405ddf30f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ENCODING LABELS\n",
            "================================================================================\n",
            "✓ Cluster classes: ['Cluster 1' 'Cluster 2' 'Cluster 3' 'Cluster 4' 'Cluster 5']\n",
            "✓ Number of clusters: 5\n",
            "\n",
            "Class distribution:\n",
            "  2: Cluster 3 - 48 samples\n",
            "  1: Cluster 2 - 44 samples\n",
            "  0: Cluster 1 - 43 samples\n",
            "  3: Cluster 4 - 33 samples\n",
            "  4: Cluster 5 - 26 samples\n",
            "\n",
            "✓ Class weights: [0.9023256  0.8818182  0.80833334 1.1757575  1.4923077 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. TRAINING & EVALUATION FUNCTIONS"
      ],
      "metadata": {
        "id": "wt3uF_BMEVfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        pitch = batch['pitch'].to(device)\n",
        "        velocity = batch['velocity'].to(device)\n",
        "        duration = batch['duration'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(pitch, velocity, duration, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Predictions\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pitch = batch['pitch'].to(device)\n",
        "            velocity = batch['velocity'].to(device)\n",
        "            duration = batch['duration'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits, _ = model(pitch, velocity, duration, attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Predictions\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1, predictions, true_labels"
      ],
      "metadata": {
        "id": "SSSX3N7iEXlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. 5-FOLD CROSS VALIDATION"
      ],
      "metadata": {
        "id": "awvddiTWEagR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "MAX_LENGTH = 512\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 30\n",
        "N_FOLDS = 5\n",
        "WEIGHT_DECAY = 0.01\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "LABEL_SMOOTHING = 0.1\n",
        "\n",
        "# Model parameters\n",
        "D_MODEL = 256\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 4\n",
        "DROPOUT = 0.3\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPERPARAMETERS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Max MIDI length: {MAX_LENGTH}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Model dimension: {D_MODEL}\")\n",
        "print(f\"Transformer heads: {NHEAD}\")\n",
        "print(f\"Transformer layers: {NUM_LAYERS}\")\n",
        "\n",
        "# Prepare data\n",
        "X = df.index.values\n",
        "y = df['label'].values\n",
        "\n",
        "print(f\"\\n✓ Total samples: {len(X)}\")\n",
        "print(f\"✓ Total clusters: {num_classes}\")\n",
        "\n",
        "# 5-Fold Cross Validation\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING 5-FOLD CROSS VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Split data\n",
        "    train_data = df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_data = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = MIDIDataset(train_data, MAX_LENGTH)\n",
        "    val_dataset = MIDIDataset(val_data, MAX_LENGTH)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize model\n",
        "    model = OrpheusEmotionClassifier(\n",
        "        num_classes=num_classes,\n",
        "        d_model=D_MODEL,\n",
        "        nhead=NHEAD,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTHING)\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    # Scheduler\n",
        "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_f1 = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, val_precision, val_recall, val_f1, _, _ = evaluate(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_f1)\n",
        "\n",
        "        # Calculate overfitting gap\n",
        "        overfit_gap = train_acc - val_acc\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "        print(f\"Overfitting Gap: {overfit_gap:.4f}\")\n",
        "\n",
        "        if overfit_gap > 0.3:\n",
        "            print(f\"  ⚠️ WARNING: Severe overfitting!\")\n",
        "\n",
        "        # Save best model and early stopping\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(model.state_dict(), f'best_orpheus_model_fold{fold+1}.pt')\n",
        "            patience_counter = 0\n",
        "            print(f\"  ✓ New best F1: {best_val_f1:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  No improvement ({patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"  Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # Load best model and final evaluation\n",
        "    model.load_state_dict(torch.load(f'best_orpheus_model_fold{fold+1}.pt'))\n",
        "    val_loss, val_acc, val_precision, val_recall, val_f1, predictions, true_labels = evaluate(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FOLD {fold + 1} FINAL RESULTS:\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Accuracy:  {val_acc:.4f}\")\n",
        "    print(f\"Precision: {val_precision:.4f}\")\n",
        "    print(f\"Recall:    {val_recall:.4f}\")\n",
        "    print(f\"F1-Score:  {val_f1:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    fold_results.append({\n",
        "        'fold': fold + 1,\n",
        "        'accuracy': val_acc,\n",
        "        'precision': val_precision,\n",
        "        'recall': val_recall,\n",
        "        'f1': val_f1\n",
        "    })\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        true_labels, predictions,\n",
        "        target_names=label_encoder.classes_,\n",
        "        digits=4,\n",
        "        zero_division=0\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnXrE-CZEcZr",
        "outputId": "6fc1fa30-0653-4427-c74f-5b952331fd1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "HYPERPARAMETERS\n",
            "================================================================================\n",
            "Batch size: 32\n",
            "Max MIDI length: 512\n",
            "Learning rate: 0.0001\n",
            "Epochs: 30\n",
            "Model dimension: 256\n",
            "Transformer heads: 8\n",
            "Transformer layers: 4\n",
            "\n",
            "✓ Total samples: 194\n",
            "✓ Total clusters: 5\n",
            "\n",
            "================================================================================\n",
            "STARTING 5-FOLD CROSS VALIDATION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "FOLD 1/5\n",
            "================================================================================\n",
            "Train size: 155, Val size: 39\n",
            "\n",
            "Epoch 1/30\n",
            "Train Loss: 1.7623, Train Acc: 0.2452\n",
            "Val Loss: 1.7748, Val Acc: 0.1795, Val F1: 0.1491\n",
            "Overfitting Gap: 0.0657\n",
            "  ✓ New best F1: 0.1491\n",
            "\n",
            "Epoch 2/30\n",
            "Train Loss: 1.6265, Train Acc: 0.2774\n",
            "Val Loss: 1.6499, Val Acc: 0.1795, Val F1: 0.1517\n",
            "Overfitting Gap: 0.0979\n",
            "  ✓ New best F1: 0.1517\n",
            "\n",
            "Epoch 3/30\n",
            "Train Loss: 1.6126, Train Acc: 0.2645\n",
            "Val Loss: 1.6638, Val Acc: 0.2308, Val F1: 0.1379\n",
            "Overfitting Gap: 0.0337\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 4/30\n",
            "Train Loss: 1.5690, Train Acc: 0.3161\n",
            "Val Loss: 1.7647, Val Acc: 0.2564, Val F1: 0.2452\n",
            "Overfitting Gap: 0.0597\n",
            "  ✓ New best F1: 0.2452\n",
            "\n",
            "Epoch 5/30\n",
            "Train Loss: 1.5462, Train Acc: 0.3290\n",
            "Val Loss: 1.9153, Val Acc: 0.2308, Val F1: 0.2015\n",
            "Overfitting Gap: 0.0983\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 6/30\n",
            "Train Loss: 1.5339, Train Acc: 0.3548\n",
            "Val Loss: 1.7873, Val Acc: 0.2564, Val F1: 0.2427\n",
            "Overfitting Gap: 0.0984\n",
            "  No improvement (2/5)\n",
            "\n",
            "Epoch 7/30\n",
            "Train Loss: 1.5795, Train Acc: 0.3032\n",
            "Val Loss: 1.7510, Val Acc: 0.2308, Val F1: 0.2499\n",
            "Overfitting Gap: 0.0725\n",
            "  ✓ New best F1: 0.2499\n",
            "\n",
            "Epoch 8/30\n",
            "Train Loss: 1.5295, Train Acc: 0.3226\n",
            "Val Loss: 1.7945, Val Acc: 0.2564, Val F1: 0.2651\n",
            "Overfitting Gap: 0.0662\n",
            "  ✓ New best F1: 0.2651\n",
            "\n",
            "Epoch 9/30\n",
            "Train Loss: 1.5338, Train Acc: 0.3161\n",
            "Val Loss: 1.8188, Val Acc: 0.2821, Val F1: 0.2688\n",
            "Overfitting Gap: 0.0341\n",
            "  ✓ New best F1: 0.2688\n",
            "\n",
            "Epoch 10/30\n",
            "Train Loss: 1.5708, Train Acc: 0.3548\n",
            "Val Loss: 1.8058, Val Acc: 0.3333, Val F1: 0.3128\n",
            "Overfitting Gap: 0.0215\n",
            "  ✓ New best F1: 0.3128\n",
            "\n",
            "Epoch 11/30\n",
            "Train Loss: 1.5167, Train Acc: 0.3613\n",
            "Val Loss: 1.7337, Val Acc: 0.2564, Val F1: 0.2644\n",
            "Overfitting Gap: 0.1049\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 12/30\n",
            "Train Loss: 1.5255, Train Acc: 0.3097\n",
            "Val Loss: 1.7153, Val Acc: 0.2821, Val F1: 0.2810\n",
            "Overfitting Gap: 0.0276\n",
            "  No improvement (2/5)\n",
            "\n",
            "Epoch 13/30\n",
            "Train Loss: 1.4526, Train Acc: 0.3419\n",
            "Val Loss: 1.7546, Val Acc: 0.3846, Val F1: 0.3712\n",
            "Overfitting Gap: -0.0427\n",
            "  ✓ New best F1: 0.3712\n",
            "\n",
            "Epoch 14/30\n",
            "Train Loss: 1.3984, Train Acc: 0.4258\n",
            "Val Loss: 1.7741, Val Acc: 0.2564, Val F1: 0.2514\n",
            "Overfitting Gap: 0.1694\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 15/30\n",
            "Train Loss: 1.4508, Train Acc: 0.3677\n",
            "Val Loss: 1.7200, Val Acc: 0.2308, Val F1: 0.2328\n",
            "Overfitting Gap: 0.1370\n",
            "  No improvement (2/5)\n",
            "\n",
            "Epoch 16/30\n",
            "Train Loss: 1.4220, Train Acc: 0.3806\n",
            "Val Loss: 1.7510, Val Acc: 0.3077, Val F1: 0.2901\n",
            "Overfitting Gap: 0.0730\n",
            "  No improvement (3/5)\n",
            "\n",
            "Epoch 17/30\n",
            "Train Loss: 1.4537, Train Acc: 0.3806\n",
            "Val Loss: 1.8108, Val Acc: 0.3333, Val F1: 0.3343\n",
            "Overfitting Gap: 0.0473\n",
            "  No improvement (4/5)\n",
            "\n",
            "Epoch 18/30\n",
            "Train Loss: 1.3715, Train Acc: 0.4452\n",
            "Val Loss: 1.8702, Val Acc: 0.2821, Val F1: 0.2587\n",
            "Overfitting Gap: 0.1631\n",
            "  No improvement (5/5)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 1 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.3846\n",
            "Precision: 0.4890\n",
            "Recall:    0.3846\n",
            "F1-Score:  0.3712\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.4444    0.4444    0.4444         9\n",
            "   Cluster 2     0.2857    0.2222    0.2500         9\n",
            "   Cluster 3     0.5000    0.6667    0.5714         9\n",
            "   Cluster 4     1.0000    0.1429    0.2500         7\n",
            "   Cluster 5     0.2000    0.4000    0.2667         5\n",
            "\n",
            "    accuracy                         0.3846        39\n",
            "   macro avg     0.4860    0.3752    0.3565        39\n",
            "weighted avg     0.4890    0.3846    0.3712        39\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 2/5\n",
            "================================================================================\n",
            "Train size: 155, Val size: 39\n",
            "\n",
            "Epoch 1/30\n",
            "Train Loss: 1.7276, Train Acc: 0.2194\n",
            "Val Loss: 1.6112, Val Acc: 0.2308, Val F1: 0.2073\n",
            "Overfitting Gap: -0.0114\n",
            "  ✓ New best F1: 0.2073\n",
            "\n",
            "Epoch 2/30\n",
            "Train Loss: 1.6408, Train Acc: 0.2581\n",
            "Val Loss: 1.7192, Val Acc: 0.2308, Val F1: 0.1611\n",
            "Overfitting Gap: 0.0273\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 3/30\n",
            "Train Loss: 1.6204, Train Acc: 0.2516\n",
            "Val Loss: 1.6783, Val Acc: 0.2308, Val F1: 0.2444\n",
            "Overfitting Gap: 0.0208\n",
            "  ✓ New best F1: 0.2444\n",
            "\n",
            "Epoch 4/30\n",
            "Train Loss: 1.6175, Train Acc: 0.2968\n",
            "Val Loss: 1.6637, Val Acc: 0.2308, Val F1: 0.2258\n",
            "Overfitting Gap: 0.0660\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 5/30\n",
            "Train Loss: 1.5841, Train Acc: 0.2774\n",
            "Val Loss: 1.6789, Val Acc: 0.2821, Val F1: 0.2817\n",
            "Overfitting Gap: -0.0046\n",
            "  ✓ New best F1: 0.2817\n",
            "\n",
            "Epoch 6/30\n",
            "Train Loss: 1.5884, Train Acc: 0.3097\n",
            "Val Loss: 1.7003, Val Acc: 0.2821, Val F1: 0.2788\n",
            "Overfitting Gap: 0.0276\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 7/30\n",
            "Train Loss: 1.5288, Train Acc: 0.3290\n",
            "Val Loss: 1.6482, Val Acc: 0.2308, Val F1: 0.1770\n",
            "Overfitting Gap: 0.0983\n",
            "  No improvement (2/5)\n",
            "\n",
            "Epoch 8/30\n",
            "Train Loss: 1.5105, Train Acc: 0.3548\n",
            "Val Loss: 1.6599, Val Acc: 0.2564, Val F1: 0.2219\n",
            "Overfitting Gap: 0.0984\n",
            "  No improvement (3/5)\n",
            "\n",
            "Epoch 9/30\n",
            "Train Loss: 1.4856, Train Acc: 0.3677\n",
            "Val Loss: 1.6992, Val Acc: 0.2564, Val F1: 0.2633\n",
            "Overfitting Gap: 0.1113\n",
            "  No improvement (4/5)\n",
            "\n",
            "Epoch 10/30\n",
            "Train Loss: 1.4584, Train Acc: 0.3806\n",
            "Val Loss: 1.7010, Val Acc: 0.2051, Val F1: 0.2115\n",
            "Overfitting Gap: 0.1755\n",
            "  No improvement (5/5)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 2 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.2821\n",
            "Precision: 0.2936\n",
            "Recall:    0.2821\n",
            "F1-Score:  0.2817\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.3750    0.3333    0.3529         9\n",
            "   Cluster 2     0.3333    0.3333    0.3333         9\n",
            "   Cluster 3     0.3077    0.4000    0.3478        10\n",
            "   Cluster 4     0.3333    0.1667    0.2222         6\n",
            "   Cluster 5     0.0000    0.0000    0.0000         5\n",
            "\n",
            "    accuracy                         0.2821        39\n",
            "   macro avg     0.2699    0.2467    0.2513        39\n",
            "weighted avg     0.2936    0.2821    0.2817        39\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 3/5\n",
            "================================================================================\n",
            "Train size: 155, Val size: 39\n",
            "\n",
            "Epoch 1/30\n",
            "Train Loss: 1.8262, Train Acc: 0.2387\n",
            "Val Loss: 1.6875, Val Acc: 0.3846, Val F1: 0.3029\n",
            "Overfitting Gap: -0.1459\n",
            "  ✓ New best F1: 0.3029\n",
            "\n",
            "Epoch 2/30\n",
            "Train Loss: 1.7715, Train Acc: 0.1871\n",
            "Val Loss: 1.5308, Val Acc: 0.2564, Val F1: 0.2025\n",
            "Overfitting Gap: -0.0693\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 3/30\n",
            "Train Loss: 1.7088, Train Acc: 0.2258\n",
            "Val Loss: 1.5230, Val Acc: 0.2821, Val F1: 0.2682\n",
            "Overfitting Gap: -0.0562\n",
            "  No improvement (2/5)\n",
            "\n",
            "Epoch 4/30\n",
            "Train Loss: 1.6269, Train Acc: 0.2968\n",
            "Val Loss: 1.5166, Val Acc: 0.3333, Val F1: 0.2841\n",
            "Overfitting Gap: -0.0366\n",
            "  No improvement (3/5)\n",
            "\n",
            "Epoch 5/30\n",
            "Train Loss: 1.7091, Train Acc: 0.2129\n",
            "Val Loss: 1.5026, Val Acc: 0.2821, Val F1: 0.2549\n",
            "Overfitting Gap: -0.0691\n",
            "  No improvement (4/5)\n",
            "\n",
            "Epoch 6/30\n",
            "Train Loss: 1.6121, Train Acc: 0.2387\n",
            "Val Loss: 1.4852, Val Acc: 0.2821, Val F1: 0.2549\n",
            "Overfitting Gap: -0.0433\n",
            "  No improvement (5/5)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 3 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.3846\n",
            "Precision: 0.2607\n",
            "Recall:    0.3846\n",
            "F1-Score:  0.3029\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.2857    0.6667    0.4000         9\n",
            "   Cluster 2     0.0000    0.0000    0.0000         9\n",
            "   Cluster 3     0.5455    0.6000    0.5714        10\n",
            "   Cluster 4     0.0000    0.0000    0.0000         6\n",
            "   Cluster 5     0.4286    0.6000    0.5000         5\n",
            "\n",
            "    accuracy                         0.3846        39\n",
            "   macro avg     0.2519    0.3733    0.2943        39\n",
            "weighted avg     0.2607    0.3846    0.3029        39\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 4/5\n",
            "================================================================================\n",
            "Train size: 155, Val size: 39\n",
            "\n",
            "Epoch 1/30\n",
            "Train Loss: 1.8177, Train Acc: 0.1484\n",
            "Val Loss: 1.6314, Val Acc: 0.3590, Val F1: 0.2491\n",
            "Overfitting Gap: -0.2106\n",
            "  ✓ New best F1: 0.2491\n",
            "\n",
            "Epoch 2/30\n",
            "Train Loss: 1.6675, Train Acc: 0.2516\n",
            "Val Loss: 1.5783, Val Acc: 0.2308, Val F1: 0.1981\n",
            "Overfitting Gap: 0.0208\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 3/30\n",
            "Train Loss: 1.6730, Train Acc: 0.2516\n",
            "Val Loss: 1.5907, Val Acc: 0.3333, Val F1: 0.2945\n",
            "Overfitting Gap: -0.0817\n",
            "  ✓ New best F1: 0.2945\n",
            "\n",
            "Epoch 4/30\n",
            "Train Loss: 1.5741, Train Acc: 0.3097\n",
            "Val Loss: 1.5695, Val Acc: 0.3846, Val F1: 0.3526\n",
            "Overfitting Gap: -0.0749\n",
            "  ✓ New best F1: 0.3526\n",
            "\n",
            "Epoch 5/30\n",
            "Train Loss: 1.5905, Train Acc: 0.2968\n",
            "Val Loss: 1.5741, Val Acc: 0.3077, Val F1: 0.2444\n",
            "Overfitting Gap: -0.0109\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 6/30\n",
            "Train Loss: 1.5715, Train Acc: 0.3484\n",
            "Val Loss: 1.5605, Val Acc: 0.3077, Val F1: 0.2708\n",
            "Overfitting Gap: 0.0407\n",
            "  No improvement (2/5)\n",
            "\n",
            "Epoch 7/30\n",
            "Train Loss: 1.5525, Train Acc: 0.3290\n",
            "Val Loss: 1.5563, Val Acc: 0.3077, Val F1: 0.2937\n",
            "Overfitting Gap: 0.0213\n",
            "  No improvement (3/5)\n",
            "\n",
            "Epoch 8/30\n",
            "Train Loss: 1.5723, Train Acc: 0.3097\n",
            "Val Loss: 1.5244, Val Acc: 0.3333, Val F1: 0.2795\n",
            "Overfitting Gap: -0.0237\n",
            "  No improvement (4/5)\n",
            "\n",
            "Epoch 9/30\n",
            "Train Loss: 1.5932, Train Acc: 0.3290\n",
            "Val Loss: 1.5426, Val Acc: 0.3077, Val F1: 0.2353\n",
            "Overfitting Gap: 0.0213\n",
            "  No improvement (5/5)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 4 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.3846\n",
            "Precision: 0.4957\n",
            "Recall:    0.3846\n",
            "F1-Score:  0.3526\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.3571    0.6250    0.4545         8\n",
            "   Cluster 2     1.0000    0.1250    0.2222         8\n",
            "   Cluster 3     0.3571    0.5000    0.4167        10\n",
            "   Cluster 4     0.3333    0.1429    0.2000         7\n",
            "   Cluster 5     0.4286    0.5000    0.4615         6\n",
            "\n",
            "    accuracy                         0.3846        39\n",
            "   macro avg     0.4952    0.3786    0.3510        39\n",
            "weighted avg     0.4957    0.3846    0.3526        39\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 5/5\n",
            "================================================================================\n",
            "Train size: 156, Val size: 38\n",
            "\n",
            "Epoch 1/30\n",
            "Train Loss: 1.7211, Train Acc: 0.2244\n",
            "Val Loss: 1.6443, Val Acc: 0.2895, Val F1: 0.2356\n",
            "Overfitting Gap: -0.0651\n",
            "  ✓ New best F1: 0.2356\n",
            "\n",
            "Epoch 2/30\n",
            "Train Loss: 1.6501, Train Acc: 0.2692\n",
            "Val Loss: 1.5761, Val Acc: 0.2895, Val F1: 0.2425\n",
            "Overfitting Gap: -0.0202\n",
            "  ✓ New best F1: 0.2425\n",
            "\n",
            "Epoch 3/30\n",
            "Train Loss: 1.6255, Train Acc: 0.3205\n",
            "Val Loss: 1.5827, Val Acc: 0.3421, Val F1: 0.3195\n",
            "Overfitting Gap: -0.0216\n",
            "  ✓ New best F1: 0.3195\n",
            "\n",
            "Epoch 4/30\n",
            "Train Loss: 1.6488, Train Acc: 0.2949\n",
            "Val Loss: 1.5985, Val Acc: 0.2895, Val F1: 0.2399\n",
            "Overfitting Gap: 0.0054\n",
            "  No improvement (1/5)\n",
            "\n",
            "Epoch 5/30\n",
            "Train Loss: 1.6297, Train Acc: 0.2885\n",
            "Val Loss: 1.5475, Val Acc: 0.2895, Val F1: 0.2424\n",
            "Overfitting Gap: -0.0010\n",
            "  No improvement (2/5)\n",
            "\n",
            "Epoch 6/30\n",
            "Train Loss: 1.5297, Train Acc: 0.3462\n",
            "Val Loss: 1.5764, Val Acc: 0.2632, Val F1: 0.2128\n",
            "Overfitting Gap: 0.0830\n",
            "  No improvement (3/5)\n",
            "\n",
            "Epoch 7/30\n",
            "Train Loss: 1.6064, Train Acc: 0.3462\n",
            "Val Loss: 1.6053, Val Acc: 0.2895, Val F1: 0.2169\n",
            "Overfitting Gap: 0.0567\n",
            "  No improvement (4/5)\n",
            "\n",
            "Epoch 8/30\n",
            "Train Loss: 1.6036, Train Acc: 0.2564\n",
            "Val Loss: 1.5561, Val Acc: 0.3421, Val F1: 0.3055\n",
            "Overfitting Gap: -0.0857\n",
            "  No improvement (5/5)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 5 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.3421\n",
            "Precision: 0.3812\n",
            "Recall:    0.3421\n",
            "F1-Score:  0.3195\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.3750    0.3750    0.3750         8\n",
            "   Cluster 2     0.3333    0.1111    0.1667         9\n",
            "   Cluster 3     0.4615    0.6667    0.5455         9\n",
            "   Cluster 4     0.5000    0.1429    0.2222         7\n",
            "   Cluster 5     0.1667    0.4000    0.2353         5\n",
            "\n",
            "    accuracy                         0.3421        38\n",
            "   macro avg     0.3673    0.3391    0.3089        38\n",
            "weighted avg     0.3812    0.3421    0.3195        38\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. FINAL RESULTS"
      ],
      "metadata": {
        "id": "llj6oJEFEf5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5-FOLD CROSS VALIDATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(fold_results)\n",
        "print(\"\\nResults per fold:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AVERAGE PERFORMANCE ACROSS ALL FOLDS:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:  {results_df['accuracy'].mean():.4f} ± {results_df['accuracy'].std():.4f}\")\n",
        "print(f\"Precision: {results_df['precision'].mean():.4f} ± {results_df['precision'].std():.4f}\")\n",
        "print(f\"Recall:    {results_df['recall'].mean():.4f} ± {results_df['recall'].std():.4f}\")\n",
        "print(f\"F1-Score:  {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\")\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('orpheus_midi_cv_results.csv', index=False)\n",
        "print(\"\\n✓ Results saved to 'orpheus_midi_cv_results.csv'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ ORPHEUS MIDI CLASSIFICATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Performance analysis\n",
        "avg_f1 = results_df['f1'].mean()\n",
        "print(f\"\\n📊 PERFORMANCE ANALYSIS:\")\n",
        "print(f\"MIDI F1-Score: {avg_f1:.2%}\")\n",
        "print(f\"Dataset size: {len(df)} samples (only 25% of lyrics dataset!)\")\n",
        "\n",
        "print(\"\\n⚠️ CRITICAL LIMITATION:\")\n",
        "print(f\"  • MIDI samples: 194 vs Lyrics: 764\")\n",
        "print(f\"  • Missing: 75% of songs have NO MIDI data!\")\n",
        "print(f\"  • Per class: ~40 samples (EXTREMELY LOW)\")\n",
        "\n",
        "if avg_f1 < 0.40:\n",
        "    print(\"\\n❌ MIDI-ONLY PERFORMANCE IS POOR\")\n",
        "    print(\"\\n🔍 ROOT CAUSES:\")\n",
        "    print(\"  1. Dataset TOO SMALL (194 samples)\")\n",
        "    print(\"     • Need 1000+ per class for Transformer\")\n",
        "    print(\"     • Currently have ~40 per class (4% of ideal)\")\n",
        "\n",
        "    print(\"\\n  2. 75% Data Missing\")\n",
        "    print(\"     • Most songs don't have MIDI files\")\n",
        "    print(\"     • Creates severe data scarcity\")\n",
        "\n",
        "    print(\"\\n  3. MIDI Alone Insufficient\")\n",
        "    print(\"     • MIDI = instrumental structure only\")\n",
        "    print(\"     • Missing: lyrics sentiment, audio timbre\")\n",
        "\n",
        "    print(\"\\n💡 REALISTIC EXPECTATIONS:\")\n",
        "    print(\"  ❌ MIDI-only: 25-35% (current - POOR)\")\n",
        "    print(\"  ⚠️ Lyrics-only: 40-55% (better)\")\n",
        "    print(\"  ✅ Audio-only: 50-60% (best single modality)\")\n",
        "    print(\"  🎯 MULTIMODAL (all 3): 60-75% (TARGET)\")\n",
        "\n",
        "    print(\"\\n🚀 RECOMMENDED APPROACH:\")\n",
        "    print(\"  1. Skip individual MIDI training (data too small)\")\n",
        "    print(\"  2. Focus on Audio modality (PANNs)\")\n",
        "    print(\"  3. Use MIDI as SUPPLEMENTARY in multimodal fusion\")\n",
        "    print(\"  4. MIDI will add ~5% when combined with Lyrics+Audio\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n✓ Decent performance given data constraints!\")\n",
        "\n",
        "print(\"\\n📈 NEXT STEPS:\")\n",
        "print(\"  1. ✓ Lyrics modality (BERT) - F1: ~45-55%\")\n",
        "print(\"  2. ✓ MIDI modality (Orpheus) - F1: ~{:.0%} (LIMITED DATA)\".format(avg_f1))\n",
        "print(\"  3. ⏳ Audio modality (PANNs) - Expected: 50-60%\")\n",
        "print(\"  4. ⏳ Multimodal fusion (Late fusion) - Expected: 60-75%\")\n",
        "\n",
        "print(\"\\n💡 STRATEGY RECOMMENDATION:\")\n",
        "print(\"  For best results:\")\n",
        "print(\"  • Use Audio as PRIMARY modality (most samples)\")\n",
        "print(\"  • Use Lyrics as SECONDARY (semantic info)\")\n",
        "print(\"  • Use MIDI as TERTIARY (supplementary when available)\")\n",
        "print(\"  • Late fusion: weighted average based on confidence\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McQkauiyEhvQ",
        "outputId": "be96cbbe-fdd1-4e6d-b2c6-f9a3ca072130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "5-FOLD CROSS VALIDATION SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Results per fold:\n",
            " fold  accuracy  precision   recall       f1\n",
            "    1  0.384615   0.489011 0.384615 0.371184\n",
            "    2  0.282051   0.293639 0.282051 0.281745\n",
            "    3  0.384615   0.260739 0.384615 0.302930\n",
            "    4  0.384615   0.495726 0.384615 0.352565\n",
            "    5  0.342105   0.381242 0.342105 0.319503\n",
            "\n",
            "================================================================================\n",
            "AVERAGE PERFORMANCE ACROSS ALL FOLDS:\n",
            "================================================================================\n",
            "Accuracy:  0.3556 ± 0.0450\n",
            "Precision: 0.3841 ± 0.1083\n",
            "Recall:    0.3556 ± 0.0450\n",
            "F1-Score:  0.3256 ± 0.0363\n",
            "\n",
            "✓ Results saved to 'orpheus_midi_cv_results.csv'\n",
            "\n",
            "================================================================================\n",
            "✅ ORPHEUS MIDI CLASSIFICATION COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "📊 PERFORMANCE ANALYSIS:\n",
            "MIDI F1-Score: 32.56%\n",
            "Dataset size: 194 samples (only 25% of lyrics dataset!)\n",
            "\n",
            "⚠️ CRITICAL LIMITATION:\n",
            "  • MIDI samples: 194 vs Lyrics: 764\n",
            "  • Missing: 75% of songs have NO MIDI data!\n",
            "  • Per class: ~40 samples (EXTREMELY LOW)\n",
            "\n",
            "❌ MIDI-ONLY PERFORMANCE IS POOR\n",
            "\n",
            "🔍 ROOT CAUSES:\n",
            "  1. Dataset TOO SMALL (194 samples)\n",
            "     • Need 1000+ per class for Transformer\n",
            "     • Currently have ~40 per class (4% of ideal)\n",
            "\n",
            "  2. 75% Data Missing\n",
            "     • Most songs don't have MIDI files\n",
            "     • Creates severe data scarcity\n",
            "\n",
            "  3. MIDI Alone Insufficient\n",
            "     • MIDI = instrumental structure only\n",
            "     • Missing: lyrics sentiment, audio timbre\n",
            "\n",
            "💡 REALISTIC EXPECTATIONS:\n",
            "  ❌ MIDI-only: 25-35% (current - POOR)\n",
            "  ⚠️ Lyrics-only: 40-55% (better)\n",
            "  ✅ Audio-only: 50-60% (best single modality)\n",
            "  🎯 MULTIMODAL (all 3): 60-75% (TARGET)\n",
            "\n",
            "🚀 RECOMMENDED APPROACH:\n",
            "  1. Skip individual MIDI training (data too small)\n",
            "  2. Focus on Audio modality (PANNs)\n",
            "  3. Use MIDI as SUPPLEMENTARY in multimodal fusion\n",
            "  4. MIDI will add ~5% when combined with Lyrics+Audio\n",
            "\n",
            "📈 NEXT STEPS:\n",
            "  1. ✓ Lyrics modality (BERT) - F1: ~45-55%\n",
            "  2. ✓ MIDI modality (Orpheus) - F1: ~33% (LIMITED DATA)\n",
            "  3. ⏳ Audio modality (PANNs) - Expected: 50-60%\n",
            "  4. ⏳ Multimodal fusion (Late fusion) - Expected: 60-75%\n",
            "\n",
            "💡 STRATEGY RECOMMENDATION:\n",
            "  For best results:\n",
            "  • Use Audio as PRIMARY modality (most samples)\n",
            "  • Use Lyrics as SECONDARY (semantic info)\n",
            "  • Use MIDI as TERTIARY (supplementary when available)\n",
            "  • Late fusion: weighted average based on confidence\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}