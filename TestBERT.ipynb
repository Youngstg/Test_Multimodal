{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9ZRgT89a5EFXgAVzzYTxC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youngstg/Test_Multimodal/blob/main/TestBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MULTIMODAL MUSIC EMOTION CLASSIFICATION\n",
        "Part 1: BERT-based Lyrics Classification dengan 5-Fold Cross Validation\n",
        "\n",
        "Dataset: MIREX Emotion Dataset dari Kaggle\n",
        "Modalitas: Lirik (Text)\n",
        "Model: BERT (bert-base-uncased)"
      ],
      "metadata": {
        "id": "59vAjGQ4xL_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. INSTALASI DAN IMPORT LIBRARY"
      ],
      "metadata": {
        "id": "rtB00Q4exmUR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGfgE5bAxHml",
        "outputId": "11749ca2-87d4-4a79-a955-ce2abc43e591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q kagglehub transformers torch scikit-learn pandas numpy\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds untuk reproducibility\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. DOWNLOAD DAN LOAD DATASET"
      ],
      "metadata": {
        "id": "_QxviUo9x1_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"imsparsh/multimodal-mirex-emotion-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Explore dataset structure\n",
        "print(\"\\n=== Dataset Structure ===\")\n",
        "for root, dirs, files in os.walk(path):\n",
        "    level = root.replace(path, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:5]:\n",
        "        print(f'{subindent}{file}')\n",
        "    if len(files) > 5:\n",
        "        print(f'{subindent}... and {len(files)-5} more files')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_TVjwNMyBWV",
        "outputId": "efcab7e9-1820-454f-b8ea-a24c58b2d129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'multimodal-mirex-emotion-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/multimodal-mirex-emotion-dataset\n",
            "\n",
            "=== Dataset Structure ===\n",
            "multimodal-mirex-emotion-dataset/\n",
            "  README.txt\n",
            "  dataset/\n",
            "    clusters.txt\n",
            "    categories.txt\n",
            "    split-by-categories-audio.bat\n",
            "    split-by-categories-lyrics.bat\n",
            "    dataset info.html\n",
            "    ... and 2 more files\n",
            "    Audio/\n",
            "      326.mp3\n",
            "      149.mp3\n",
            "      898.mp3\n",
            "      011.mp3\n",
            "      434.mp3\n",
            "      ... and 898 more files\n",
            "    MIDIs/\n",
            "      552.mid\n",
            "      197.mid\n",
            "      019.mid\n",
            "      662.mid\n",
            "      773.mid\n",
            "      ... and 191 more files\n",
            "    Lyrics/\n",
            "      559.txt\n",
            "      557.txt\n",
            "      361.txt\n",
            "      812.txt\n",
            "      245.txt\n",
            "      ... and 759 more files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. LOAD DAN PREPROCESSING DATA"
      ],
      "metadata": {
        "id": "2d7XmgCKyFl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mirex_dataset(dataset_path):\n",
        "    \"\"\"\n",
        "    Load MIREX dataset berdasarkan struktur:\n",
        "    - categories.txt: emotion labels (satu per baris, index = line number)\n",
        "    - clusters.txt: cluster labels (satu per baris, index = line number)\n",
        "    - Lyrics/*.txt: lyric files (filename = song number)\n",
        "\n",
        "    Format sebenarnya:\n",
        "    - categories.txt: baris ke-i = emotion untuk song ke-i\n",
        "    - clusters.txt: baris ke-i = CLUSTER untuk song ke-i (GUNAKAN INI!)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Load categories (emotion labels) - for reference only\n",
        "    categories_path = os.path.join(dataset_path, 'dataset', 'categories.txt')\n",
        "    emotion_labels = []\n",
        "\n",
        "    print(\"\\n--- Loading categories.txt (reference only) ---\")\n",
        "\n",
        "    if os.path.exists(categories_path):\n",
        "        with open(categories_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "            emotion_labels = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "        print(f\"✓ Loaded {len(emotion_labels)} emotion labels\")\n",
        "        print(f\"  Unique emotions: {sorted(set(emotion_labels))}\")\n",
        "\n",
        "    # 2. Load clusters - USE THIS AS LABELS!\n",
        "    clusters_path = os.path.join(dataset_path, 'dataset', 'clusters.txt')\n",
        "    cluster_labels = []\n",
        "\n",
        "    print(\"\\n--- Loading clusters.txt (USING THIS AS LABELS) ---\")\n",
        "\n",
        "    if os.path.exists(clusters_path):\n",
        "        with open(clusters_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "            cluster_labels = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        print(f\"✓ Loaded {len(cluster_labels)} cluster labels\")\n",
        "        print(f\"  Unique clusters: {unique_clusters}\")\n",
        "        print(f\"  Number of clusters: {len(unique_clusters)}\")\n",
        "\n",
        "        # Show distribution\n",
        "        from collections import Counter\n",
        "        cluster_counts = Counter(cluster_labels)\n",
        "        print(f\"\\n  Cluster distribution:\")\n",
        "        for cluster, count in sorted(cluster_counts.items()):\n",
        "            print(f\"    {cluster}: {count} songs\")\n",
        "    else:\n",
        "        print(\"❌ clusters.txt not found!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 3. Create song_id to cluster mapping\n",
        "    song_cluster_map = {}\n",
        "\n",
        "    # Map based on index\n",
        "    for idx in range(len(cluster_labels)):\n",
        "        # Song IDs might be 0-indexed or 1-indexed, we'll try both\n",
        "        song_id_0 = str(idx).zfill(3)  # 000, 001, 002...\n",
        "        song_id_1 = str(idx + 1).zfill(3)  # 001, 002, 003...\n",
        "\n",
        "        song_cluster_map[song_id_0] = cluster_labels[idx]\n",
        "        song_cluster_map[song_id_1] = cluster_labels[idx]\n",
        "\n",
        "    print(f\"\\n✓ Created mappings for {len(song_cluster_map)} potential song IDs\")\n",
        "\n",
        "    # 4. Load lyrics\n",
        "    lyrics_dir = os.path.join(dataset_path, 'dataset', 'Lyrics')\n",
        "    data = []\n",
        "\n",
        "    if os.path.exists(lyrics_dir):\n",
        "        lyrics_files = [f for f in os.listdir(lyrics_dir) if f.endswith('.txt')]\n",
        "        print(f\"\\n--- Loading lyrics ---\")\n",
        "        print(f\"✓ Found {len(lyrics_files)} lyric files\")\n",
        "\n",
        "        # Sample some filenames to understand naming\n",
        "        print(f\"  Sample filenames: {sorted(lyrics_files)[:5]}\")\n",
        "\n",
        "        matched = 0\n",
        "        unmatched = 0\n",
        "        unmatched_samples = []\n",
        "\n",
        "        for filename in lyrics_files:\n",
        "            song_id = filename.replace('.txt', '')\n",
        "\n",
        "            # Get cluster label\n",
        "            if song_id not in song_cluster_map:\n",
        "                unmatched += 1\n",
        "                if len(unmatched_samples) < 3:\n",
        "                    unmatched_samples.append(song_id)\n",
        "                continue\n",
        "\n",
        "            matched += 1\n",
        "            cluster = song_cluster_map[song_id]\n",
        "\n",
        "            # Read lyrics\n",
        "            lyrics_path = os.path.join(lyrics_dir, filename)\n",
        "            try:\n",
        "                with open(lyrics_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                    lyrics = f.read().strip()\n",
        "\n",
        "                if lyrics:  # Only add if lyrics not empty\n",
        "                    data.append({\n",
        "                        'song_id': song_id,\n",
        "                        'lyrics': lyrics,\n",
        "                        'cluster': cluster  # Changed from 'emotion' to 'cluster'\n",
        "                    })\n",
        "\n",
        "                    if len(data) <= 3:\n",
        "                        print(f\"  ✓ Loaded: {song_id} - {cluster} ({len(lyrics)} chars)\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Error reading {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n✓ Successfully matched: {matched} songs\")\n",
        "        if unmatched > 0:\n",
        "            print(f\"⚠️ Unmatched: {unmatched} songs\")\n",
        "            print(f\"  Sample unmatched IDs: {unmatched_samples}\")\n",
        "    else:\n",
        "        print(\"❌ Lyrics directory not found!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"✓ Final dataset: {len(df)} songs with lyrics and clusters\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING MIREX DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df = load_mirex_dataset(path)\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "\n",
        "if len(df) > 0:\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nCluster distribution:\")\n",
        "    print(df['cluster'].value_counts())\n",
        "else:\n",
        "    print(\"\\n⚠️ ERROR: No data loaded!\")\n",
        "    print(\"Please check the debug output above to identify the issue.\")\n",
        "    raise ValueError(\"Failed to load dataset - check debug output above\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT5IUdOMyGUH",
        "outputId": "699d6b0d-49cf-4455-fdc9-bfb68b7c3305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING MIREX DATASET\n",
            "================================================================================\n",
            "\n",
            "--- Loading categories.txt (reference only) ---\n",
            "✓ Loaded 903 emotion labels\n",
            "  Unique emotions: ['Agressive', 'Amiable-good natured', 'Autumnal', 'Bittersweet', 'Boisterous', 'Brooding', 'Campy', 'Cheerful', 'Confident', 'Fiery', 'Fun', 'Humorous', 'Intense', 'Literate', 'Passionate', 'Poignant', 'Rollicking', 'Rousing', 'Rowdy', 'Silly', 'Sweet', 'Tense - Anxious', 'Visceral', 'Volatile', 'Wistful', 'Witty', 'Wry', 'whimsical']\n",
            "\n",
            "--- Loading clusters.txt (USING THIS AS LABELS) ---\n",
            "✓ Loaded 903 cluster labels\n",
            "  Unique clusters: ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5']\n",
            "  Number of clusters: 5\n",
            "\n",
            "  Cluster distribution:\n",
            "    Cluster 1: 170 songs\n",
            "    Cluster 2: 164 songs\n",
            "    Cluster 3: 215 songs\n",
            "    Cluster 4: 191 songs\n",
            "    Cluster 5: 163 songs\n",
            "\n",
            "✓ Created mappings for 904 potential song IDs\n",
            "\n",
            "--- Loading lyrics ---\n",
            "✓ Found 764 lyric files\n",
            "  Sample filenames: ['001.txt', '003.txt', '004.txt', '007.txt', '008.txt']\n",
            "  ✓ Loaded: 559 - Cluster 4 (4964 chars)\n",
            "  ✓ Loaded: 557 - Cluster 4 (156 chars)\n",
            "  ✓ Loaded: 361 - Cluster 3 (557 chars)\n",
            "\n",
            "✓ Successfully matched: 764 songs\n",
            "\n",
            "================================================================================\n",
            "✓ Final dataset: 764 songs with lyrics and clusters\n",
            "================================================================================\n",
            "\n",
            "Dataset shape: (764, 3)\n",
            "\n",
            "First few rows:\n",
            "  song_id                                             lyrics    cluster\n",
            "0     559  I remember every little thing\\nAs if it happen...  Cluster 4\n",
            "1     557  Oh Yeah... Oh Yeah... Oh Yeah\\nThe moon... bea...  Cluster 4\n",
            "2     361  In South Carolina there are many tall pines\\nI...  Cluster 3\n",
            "3     812  What is this that stands before me?\\nFigure in...  Cluster 5\n",
            "4     245  I'm breakin' rocks in the hot sun\\nI fought th...  Cluster 2\n",
            "\n",
            "Cluster distribution:\n",
            "cluster\n",
            "Cluster 3    192\n",
            "Cluster 4    173\n",
            "Cluster 2    138\n",
            "Cluster 1    134\n",
            "Cluster 5    127\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. TEXT PREPROCESSING"
      ],
      "metadata": {
        "id": "jowsfmh4yZpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_lyrics(text):\n",
        "    \"\"\"\n",
        "    Improved cleaning for lyrics text\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove common noise patterns in lyrics\n",
        "    # Remove [Chorus], [Verse], etc.\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    # Remove (x2), (repeat), etc.\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra whitespace and newlines\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Keep letters, numbers, and basic punctuation\n",
        "    text = re.sub(r'[^a-z0-9\\s.,!?\\']', ' ', text)\n",
        "\n",
        "    # Remove repeated punctuation\n",
        "    text = re.sub(r'([.,!?])\\1+', r'\\1', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Apply cleaning\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPROCESSING LYRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df['lyrics_clean'] = df['lyrics'].apply(clean_lyrics)\n",
        "print(\"✓ Cleaning completed!\")\n",
        "\n",
        "# Remove rows with empty lyrics\n",
        "df = df[df['lyrics_clean'].str.len() > 0].reset_index(drop=True)\n",
        "print(f\"✓ Dataset after removing empty lyrics: {df.shape}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample cleaned lyrics:\")\n",
        "print(df.iloc[0]['lyrics_clean'][:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgUITYN8ymPd",
        "outputId": "07f65d3c-7c72-4990-ca4a-35199851aa8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PREPROCESSING LYRICS\n",
            "================================================================================\n",
            "✓ Cleaning completed!\n",
            "✓ Dataset after removing empty lyrics: (764, 4)\n",
            "\n",
            "Sample cleaned lyrics:\n",
            "i remember every little thing as if it happened only yesterday parking by the lake and there was not another car in sight and i never had a girl looking any better than you did and all the kids at sch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. LABEL ENCODING"
      ],
      "metadata": {
        "id": "-jjDYNj1ypoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENCODING LABELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Encode cluster labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['cluster_encoded'] = label_encoder.fit_transform(df['cluster'])\n",
        "\n",
        "print(f\"✓ Cluster classes: {label_encoder.classes_}\")\n",
        "print(f\"✓ Number of clusters: {len(label_encoder.classes_)}\")\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "for cluster, count in df['cluster'].value_counts().items():\n",
        "    encoded = df[df['cluster'] == cluster]['cluster_encoded'].iloc[0]\n",
        "    print(f\"  {encoded}: {cluster} - {count} samples ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Check class imbalance\n",
        "min_samples = df['cluster'].value_counts().min()\n",
        "max_samples = df['cluster'].value_counts().max()\n",
        "imbalance_ratio = max_samples / min_samples\n",
        "print(f\"\\n⚠️ Class imbalance ratio: {imbalance_ratio:.2f}x\")\n",
        "if imbalance_ratio > 3:\n",
        "    print(\"  High class imbalance detected! Consider using class weights.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl-H42KxysC3",
        "outputId": "a9f7ff61-03a0-48d2-e518-eaa3925b543b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ENCODING LABELS\n",
            "================================================================================\n",
            "✓ Cluster classes: ['Cluster 1' 'Cluster 2' 'Cluster 3' 'Cluster 4' 'Cluster 5']\n",
            "✓ Number of clusters: 5\n",
            "\n",
            "Class distribution:\n",
            "  2: Cluster 3 - 192 samples (25.1%)\n",
            "  3: Cluster 4 - 173 samples (22.6%)\n",
            "  1: Cluster 2 - 138 samples (18.1%)\n",
            "  0: Cluster 1 - 134 samples (17.5%)\n",
            "  4: Cluster 5 - 127 samples (16.6%)\n",
            "\n",
            "⚠️ Class imbalance ratio: 1.51x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. DATASET CLASS"
      ],
      "metadata": {
        "id": "9jq8ZhKzyyJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "lZUloUfQy0-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. MODEL DEFINITION"
      ],
      "metadata": {
        "id": "s0oHNtXDy3Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTEmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, dropout=0.5):  # Increased dropout\n",
        "        super(BERTEmotionClassifier, self).__init__()\n",
        "\n",
        "        # BERT encoder\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Freeze early BERT layers to prevent overfitting\n",
        "        for param in self.bert.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for i, layer in enumerate(self.bert.encoder.layer[:8]):  # Freeze first 8 layers\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Simpler classifier head to prevent overfitting\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "        # Layer normalization for stability\n",
        "        self.layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # BERT encoding\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Use [CLS] token representation\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Normalize and classify\n",
        "        x = self.layer_norm(pooled_output)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "WxIeLgw5y5Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. TRAINING FUNCTION"
      ],
      "metadata": {
        "id": "DRZih7aby_Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, accumulation_steps=1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Normalize loss for gradient accumulation\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights every accumulation_steps\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        # Predictions\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Predictions\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    return avg_loss, accuracy, precision, recall, f1, predictions, true_labels"
      ],
      "metadata": {
        "id": "OySXGNVny_6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. 5-FOLD CROSS VALIDATION"
      ],
      "metadata": {
        "id": "lqGp2vynzGsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved Hyperparameters to prevent overfitting\n",
        "BATCH_SIZE = 16  # Increased back for stability\n",
        "MAX_LENGTH = 256  # Shorter sequences\n",
        "LEARNING_RATE = 3e-5  # Slightly higher LR for better convergence\n",
        "NUM_EPOCHS = 15  # More epochs\n",
        "N_FOLDS = 5\n",
        "WEIGHT_DECAY = 0.01  # L2 regularization\n",
        "EARLY_STOPPING_PATIENCE = 4  # More patience\n",
        "ACCUMULATION_STEPS = 2  # Gradient accumulation for larger effective batch\n",
        "\n",
        "# Initialize tokenizer\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INITIALIZING BERT TOKENIZER\")\n",
        "print(\"=\"*80)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(\"✓ Tokenizer loaded!\")\n",
        "\n",
        "# Prepare data for cross-validation\n",
        "X = df['lyrics_clean'].values\n",
        "y = df['cluster_encoded'].values\n",
        "\n",
        "print(f\"\\n✓ Total samples: {len(X)}\")\n",
        "print(f\"✓ Total clusters: {num_classes}\")\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "print(f\"\\n✓ Using class weights to handle imbalance\")\n",
        "print(f\"  Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "# 5-Fold Stratified Cross Validation\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING 5-FOLD CROSS VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Store results\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = LyricsDataset(X_train, y_train, tokenizer, MAX_LENGTH)\n",
        "    val_dataset = LyricsDataset(X_val, y_val, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize model\n",
        "    model = BERTEmotionClassifier(num_classes=num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss with class weights and optimizer with weight decay\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    # Scheduler - cosine annealing for better convergence\n",
        "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader) * NUM_EPOCHS)\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    best_val_f1 = 0\n",
        "    patience_counter = 0\n",
        "    training_history = []\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, scheduler, device, ACCUMULATION_STEPS\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc, val_precision, val_recall, val_f1, _, _ = evaluate(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Track history\n",
        "        training_history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'val_f1': val_f1\n",
        "        })\n",
        "\n",
        "        # Save best model and early stopping\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(model.state_dict(), f'best_model_fold{fold+1}.pt')\n",
        "            patience_counter = 0\n",
        "            print(f\"  ✓ New best F1: {best_val_f1:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  No improvement ({patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"  Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    # Load best model and final evaluation\n",
        "    model.load_state_dict(torch.load(f'best_model_fold{fold+1}.pt'))\n",
        "    val_loss, val_acc, val_precision, val_recall, val_f1, predictions, true_labels = evaluate(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FOLD {fold + 1} FINAL RESULTS:\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Accuracy:  {val_acc:.4f}\")\n",
        "    print(f\"Precision: {val_precision:.4f}\")\n",
        "    print(f\"Recall:    {val_recall:.4f}\")\n",
        "    print(f\"F1-Score:  {val_f1:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    fold_results.append({\n",
        "        'fold': fold + 1,\n",
        "        'accuracy': val_acc,\n",
        "        'precision': val_precision,\n",
        "        'recall': val_recall,\n",
        "        'f1': val_f1\n",
        "    })\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        true_labels, predictions,\n",
        "        target_names=label_encoder.classes_,\n",
        "        digits=4,\n",
        "        zero_division=0\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aL1KIwnzGJq",
        "outputId": "478cedf2-4bbf-4684-8c8e-fdb06096fc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "INITIALIZING BERT TOKENIZER\n",
            "================================================================================\n",
            "✓ Tokenizer loaded!\n",
            "\n",
            "✓ Total samples: 764\n",
            "✓ Total clusters: 5\n",
            "\n",
            "✓ Using class weights to handle imbalance\n",
            "  Class weights: [1.1402985  1.1072464  0.79583335 0.883237   1.2031496 ]\n",
            "\n",
            "================================================================================\n",
            "STARTING 5-FOLD CROSS VALIDATION\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "FOLD 1/5\n",
            "================================================================================\n",
            "Train size: 611, Val size: 153\n",
            "\n",
            "Epoch 1/15\n",
            "Train Loss: 1.7425, Train Acc: 0.2193\n",
            "Val Loss: 1.5796, Val Acc: 0.3268, Val F1: 0.2450\n",
            "  ✓ New best F1: 0.2450\n",
            "\n",
            "Epoch 2/15\n",
            "Train Loss: 1.6372, Train Acc: 0.2700\n",
            "Val Loss: 1.4971, Val Acc: 0.3791, Val F1: 0.3416\n",
            "  ✓ New best F1: 0.3416\n",
            "\n",
            "Epoch 3/15\n",
            "Train Loss: 1.4997, Train Acc: 0.3764\n",
            "Val Loss: 1.4945, Val Acc: 0.3464, Val F1: 0.3144\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 4/15\n",
            "Train Loss: 1.4277, Train Acc: 0.4173\n",
            "Val Loss: 1.4333, Val Acc: 0.3856, Val F1: 0.3846\n",
            "  ✓ New best F1: 0.3846\n",
            "\n",
            "Epoch 5/15\n",
            "Train Loss: 1.3153, Train Acc: 0.4566\n",
            "Val Loss: 1.4241, Val Acc: 0.4314, Val F1: 0.4081\n",
            "  ✓ New best F1: 0.4081\n",
            "\n",
            "Epoch 6/15\n",
            "Train Loss: 1.2427, Train Acc: 0.5221\n",
            "Val Loss: 1.4755, Val Acc: 0.4118, Val F1: 0.3887\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 7/15\n",
            "Train Loss: 1.0941, Train Acc: 0.5794\n",
            "Val Loss: 1.4795, Val Acc: 0.4118, Val F1: 0.4019\n",
            "  No improvement (2/4)\n",
            "\n",
            "Epoch 8/15\n",
            "Train Loss: 0.9313, Train Acc: 0.6514\n",
            "Val Loss: 1.4891, Val Acc: 0.4379, Val F1: 0.4335\n",
            "  ✓ New best F1: 0.4335\n",
            "\n",
            "Epoch 9/15\n",
            "Train Loss: 0.8853, Train Acc: 0.6759\n",
            "Val Loss: 1.5466, Val Acc: 0.3856, Val F1: 0.3843\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 10/15\n",
            "Train Loss: 0.7242, Train Acc: 0.7480\n",
            "Val Loss: 1.6511, Val Acc: 0.3922, Val F1: 0.3849\n",
            "  No improvement (2/4)\n",
            "\n",
            "Epoch 11/15\n",
            "Train Loss: 0.5834, Train Acc: 0.8134\n",
            "Val Loss: 1.6839, Val Acc: 0.3922, Val F1: 0.3948\n",
            "  No improvement (3/4)\n",
            "\n",
            "Epoch 12/15\n",
            "Train Loss: 0.5124, Train Acc: 0.8396\n",
            "Val Loss: 1.7101, Val Acc: 0.4183, Val F1: 0.4213\n",
            "  No improvement (4/4)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 1 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.4379\n",
            "Precision: 0.4406\n",
            "Recall:    0.4379\n",
            "F1-Score:  0.4335\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.3158    0.2222    0.2609        27\n",
            "   Cluster 2     0.2895    0.4074    0.3385        27\n",
            "   Cluster 3     0.5556    0.6579    0.6024        38\n",
            "   Cluster 4     0.4667    0.4000    0.4308        35\n",
            "   Cluster 5     0.5238    0.4231    0.4681        26\n",
            "\n",
            "    accuracy                         0.4379       153\n",
            "   macro avg     0.4303    0.4221    0.4201       153\n",
            "weighted avg     0.4406    0.4379    0.4335       153\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 2/5\n",
            "================================================================================\n",
            "Train size: 611, Val size: 153\n",
            "\n",
            "Epoch 1/15\n",
            "Train Loss: 1.7738, Train Acc: 0.2193\n",
            "Val Loss: 1.6031, Val Acc: 0.2353, Val F1: 0.1177\n",
            "  ✓ New best F1: 0.1177\n",
            "\n",
            "Epoch 2/15\n",
            "Train Loss: 1.6616, Train Acc: 0.2602\n",
            "Val Loss: 1.5074, Val Acc: 0.3595, Val F1: 0.2942\n",
            "  ✓ New best F1: 0.2942\n",
            "\n",
            "Epoch 3/15\n",
            "Train Loss: 1.5614, Train Acc: 0.3159\n",
            "Val Loss: 1.3824, Val Acc: 0.4183, Val F1: 0.4003\n",
            "  ✓ New best F1: 0.4003\n",
            "\n",
            "Epoch 4/15\n",
            "Train Loss: 1.4121, Train Acc: 0.4239\n",
            "Val Loss: 1.3885, Val Acc: 0.4118, Val F1: 0.4089\n",
            "  ✓ New best F1: 0.4089\n",
            "\n",
            "Epoch 5/15\n",
            "Train Loss: 1.2497, Train Acc: 0.5074\n",
            "Val Loss: 1.3756, Val Acc: 0.4118, Val F1: 0.4178\n",
            "  ✓ New best F1: 0.4178\n",
            "\n",
            "Epoch 6/15\n",
            "Train Loss: 1.1185, Train Acc: 0.5614\n",
            "Val Loss: 1.3952, Val Acc: 0.4444, Val F1: 0.4459\n",
            "  ✓ New best F1: 0.4459\n",
            "\n",
            "Epoch 7/15\n",
            "Train Loss: 1.0212, Train Acc: 0.6219\n",
            "Val Loss: 1.4028, Val Acc: 0.4444, Val F1: 0.4499\n",
            "  ✓ New best F1: 0.4499\n",
            "\n",
            "Epoch 8/15\n",
            "Train Loss: 0.9262, Train Acc: 0.6530\n",
            "Val Loss: 1.4345, Val Acc: 0.4771, Val F1: 0.4801\n",
            "  ✓ New best F1: 0.4801\n",
            "\n",
            "Epoch 9/15\n",
            "Train Loss: 0.7253, Train Acc: 0.7463\n",
            "Val Loss: 1.5060, Val Acc: 0.5033, Val F1: 0.4929\n",
            "  ✓ New best F1: 0.4929\n",
            "\n",
            "Epoch 10/15\n",
            "Train Loss: 0.6605, Train Acc: 0.7676\n",
            "Val Loss: 1.5639, Val Acc: 0.5163, Val F1: 0.5135\n",
            "  ✓ New best F1: 0.5135\n",
            "\n",
            "Epoch 11/15\n",
            "Train Loss: 0.5408, Train Acc: 0.8118\n",
            "Val Loss: 1.5501, Val Acc: 0.4902, Val F1: 0.4860\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 12/15\n",
            "Train Loss: 0.4176, Train Acc: 0.8707\n",
            "Val Loss: 1.6536, Val Acc: 0.5163, Val F1: 0.5082\n",
            "  No improvement (2/4)\n",
            "\n",
            "Epoch 13/15\n",
            "Train Loss: 0.3184, Train Acc: 0.9100\n",
            "Val Loss: 1.7039, Val Acc: 0.4902, Val F1: 0.4837\n",
            "  No improvement (3/4)\n",
            "\n",
            "Epoch 14/15\n",
            "Train Loss: 0.2679, Train Acc: 0.9313\n",
            "Val Loss: 1.8337, Val Acc: 0.5294, Val F1: 0.5261\n",
            "  ✓ New best F1: 0.5261\n",
            "\n",
            "Epoch 15/15\n",
            "Train Loss: 0.2046, Train Acc: 0.9525\n",
            "Val Loss: 1.8953, Val Acc: 0.5163, Val F1: 0.5157\n",
            "  No improvement (1/4)\n",
            "\n",
            "================================================================================\n",
            "FOLD 2 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.5294\n",
            "Precision: 0.5273\n",
            "Recall:    0.5294\n",
            "F1-Score:  0.5261\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.5238    0.4074    0.4583        27\n",
            "   Cluster 2     0.5862    0.6296    0.6071        27\n",
            "   Cluster 3     0.5455    0.6316    0.5854        38\n",
            "   Cluster 4     0.5000    0.4857    0.4928        35\n",
            "   Cluster 5     0.4800    0.4615    0.4706        26\n",
            "\n",
            "    accuracy                         0.5294       153\n",
            "   macro avg     0.5271    0.5232    0.5228       153\n",
            "weighted avg     0.5273    0.5294    0.5261       153\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 3/5\n",
            "================================================================================\n",
            "Train size: 611, Val size: 153\n",
            "\n",
            "Epoch 1/15\n",
            "Train Loss: 1.7932, Train Acc: 0.2029\n",
            "Val Loss: 1.5990, Val Acc: 0.1961, Val F1: 0.1068\n",
            "  ✓ New best F1: 0.1068\n",
            "\n",
            "Epoch 2/15\n",
            "Train Loss: 1.6900, Train Acc: 0.2079\n",
            "Val Loss: 1.5355, Val Acc: 0.2614, Val F1: 0.1883\n",
            "  ✓ New best F1: 0.1883\n",
            "\n",
            "Epoch 3/15\n",
            "Train Loss: 1.5498, Train Acc: 0.3159\n",
            "Val Loss: 1.4678, Val Acc: 0.4183, Val F1: 0.3592\n",
            "  ✓ New best F1: 0.3592\n",
            "\n",
            "Epoch 4/15\n",
            "Train Loss: 1.4417, Train Acc: 0.3928\n",
            "Val Loss: 1.3990, Val Acc: 0.4052, Val F1: 0.3825\n",
            "  ✓ New best F1: 0.3825\n",
            "\n",
            "Epoch 5/15\n",
            "Train Loss: 1.2567, Train Acc: 0.5090\n",
            "Val Loss: 1.4437, Val Acc: 0.4248, Val F1: 0.4210\n",
            "  ✓ New best F1: 0.4210\n",
            "\n",
            "Epoch 6/15\n",
            "Train Loss: 1.1321, Train Acc: 0.5516\n",
            "Val Loss: 1.4733, Val Acc: 0.4902, Val F1: 0.4723\n",
            "  ✓ New best F1: 0.4723\n",
            "\n",
            "Epoch 7/15\n",
            "Train Loss: 1.0057, Train Acc: 0.6285\n",
            "Val Loss: 1.5624, Val Acc: 0.4052, Val F1: 0.4091\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 8/15\n",
            "Train Loss: 0.8660, Train Acc: 0.6939\n",
            "Val Loss: 1.6353, Val Acc: 0.4379, Val F1: 0.4189\n",
            "  No improvement (2/4)\n",
            "\n",
            "Epoch 9/15\n",
            "Train Loss: 0.7895, Train Acc: 0.7136\n",
            "Val Loss: 1.6851, Val Acc: 0.4118, Val F1: 0.4146\n",
            "  No improvement (3/4)\n",
            "\n",
            "Epoch 10/15\n",
            "Train Loss: 0.6895, Train Acc: 0.7529\n",
            "Val Loss: 1.7285, Val Acc: 0.4314, Val F1: 0.4253\n",
            "  No improvement (4/4)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 3 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.4902\n",
            "Precision: 0.4899\n",
            "Recall:    0.4902\n",
            "F1-Score:  0.4723\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.4500    0.3333    0.3830        27\n",
            "   Cluster 2     0.4400    0.3929    0.4151        28\n",
            "   Cluster 3     0.5091    0.7368    0.6022        38\n",
            "   Cluster 4     0.5000    0.6000    0.5455        35\n",
            "   Cluster 5     0.5455    0.2400    0.3333        25\n",
            "\n",
            "    accuracy                         0.4902       153\n",
            "   macro avg     0.4889    0.4606    0.4558       153\n",
            "weighted avg     0.4899    0.4902    0.4723       153\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 4/5\n",
            "================================================================================\n",
            "Train size: 611, Val size: 153\n",
            "\n",
            "Epoch 1/15\n",
            "Train Loss: 1.7276, Train Acc: 0.2373\n",
            "Val Loss: 1.5827, Val Acc: 0.2222, Val F1: 0.1317\n",
            "  ✓ New best F1: 0.1317\n",
            "\n",
            "Epoch 2/15\n",
            "Train Loss: 1.7044, Train Acc: 0.2259\n",
            "Val Loss: 1.4760, Val Acc: 0.4771, Val F1: 0.4319\n",
            "  ✓ New best F1: 0.4319\n",
            "\n",
            "Epoch 3/15\n",
            "Train Loss: 1.5132, Train Acc: 0.3666\n",
            "Val Loss: 1.3655, Val Acc: 0.4837, Val F1: 0.4801\n",
            "  ✓ New best F1: 0.4801\n",
            "\n",
            "Epoch 4/15\n",
            "Train Loss: 1.4742, Train Acc: 0.3470\n",
            "Val Loss: 1.3027, Val Acc: 0.4510, Val F1: 0.4256\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 5/15\n",
            "Train Loss: 1.3165, Train Acc: 0.4845\n",
            "Val Loss: 1.2639, Val Acc: 0.4706, Val F1: 0.4575\n",
            "  No improvement (2/4)\n",
            "\n",
            "Epoch 6/15\n",
            "Train Loss: 1.1960, Train Acc: 0.5876\n",
            "Val Loss: 1.2507, Val Acc: 0.4771, Val F1: 0.4770\n",
            "  No improvement (3/4)\n",
            "\n",
            "Epoch 7/15\n",
            "Train Loss: 1.0730, Train Acc: 0.5941\n",
            "Val Loss: 1.2941, Val Acc: 0.4183, Val F1: 0.4218\n",
            "  No improvement (4/4)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 4 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.4837\n",
            "Precision: 0.5159\n",
            "Recall:    0.4837\n",
            "F1-Score:  0.4801\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.3846    0.1852    0.2500        27\n",
            "   Cluster 2     0.3333    0.5714    0.4211        28\n",
            "   Cluster 3     0.7059    0.6154    0.6575        39\n",
            "   Cluster 4     0.4419    0.5588    0.4935        34\n",
            "   Cluster 5     0.6667    0.4000    0.5000        25\n",
            "\n",
            "    accuracy                         0.4837       153\n",
            "   macro avg     0.5065    0.4662    0.4644       153\n",
            "weighted avg     0.5159    0.4837    0.4801       153\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 5/5\n",
            "================================================================================\n",
            "Train size: 612, Val size: 152\n",
            "\n",
            "Epoch 1/15\n",
            "Train Loss: 1.7750, Train Acc: 0.2075\n",
            "Val Loss: 1.5874, Val Acc: 0.2368, Val F1: 0.1748\n",
            "  ✓ New best F1: 0.1748\n",
            "\n",
            "Epoch 2/15\n",
            "Train Loss: 1.6476, Train Acc: 0.2598\n",
            "Val Loss: 1.5128, Val Acc: 0.2961, Val F1: 0.2717\n",
            "  ✓ New best F1: 0.2717\n",
            "\n",
            "Epoch 3/15\n",
            "Train Loss: 1.4630, Train Acc: 0.3693\n",
            "Val Loss: 1.5273, Val Acc: 0.2829, Val F1: 0.2447\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 4/15\n",
            "Train Loss: 1.4403, Train Acc: 0.3954\n",
            "Val Loss: 1.4634, Val Acc: 0.3947, Val F1: 0.3581\n",
            "  ✓ New best F1: 0.3581\n",
            "\n",
            "Epoch 5/15\n",
            "Train Loss: 1.3123, Train Acc: 0.4575\n",
            "Val Loss: 1.5271, Val Acc: 0.3618, Val F1: 0.3541\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 6/15\n",
            "Train Loss: 1.1797, Train Acc: 0.5261\n",
            "Val Loss: 1.5203, Val Acc: 0.4276, Val F1: 0.4339\n",
            "  ✓ New best F1: 0.4339\n",
            "\n",
            "Epoch 7/15\n",
            "Train Loss: 1.1544, Train Acc: 0.5359\n",
            "Val Loss: 1.4993, Val Acc: 0.3882, Val F1: 0.3733\n",
            "  No improvement (1/4)\n",
            "\n",
            "Epoch 8/15\n",
            "Train Loss: 1.0108, Train Acc: 0.6078\n",
            "Val Loss: 1.7321, Val Acc: 0.4408, Val F1: 0.4223\n",
            "  No improvement (2/4)\n",
            "\n",
            "Epoch 9/15\n",
            "Train Loss: 0.9069, Train Acc: 0.6716\n",
            "Val Loss: 1.7108, Val Acc: 0.3816, Val F1: 0.3724\n",
            "  No improvement (3/4)\n",
            "\n",
            "Epoch 10/15\n",
            "Train Loss: 0.7261, Train Acc: 0.7271\n",
            "Val Loss: 1.7778, Val Acc: 0.3487, Val F1: 0.3358\n",
            "  No improvement (4/4)\n",
            "  Early stopping triggered!\n",
            "\n",
            "================================================================================\n",
            "FOLD 5 FINAL RESULTS:\n",
            "================================================================================\n",
            "Accuracy:  0.4276\n",
            "Precision: 0.4920\n",
            "Recall:    0.4276\n",
            "F1-Score:  0.4339\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.2500    0.1923    0.2174        26\n",
            "   Cluster 2     0.5000    0.3929    0.4400        28\n",
            "   Cluster 3     0.6000    0.4615    0.5217        39\n",
            "   Cluster 4     0.7000    0.4118    0.5185        34\n",
            "   Cluster 5     0.2833    0.6800    0.4000        25\n",
            "\n",
            "    accuracy                         0.4276       152\n",
            "   macro avg     0.4667    0.4277    0.4195       152\n",
            "weighted avg     0.4920    0.4276    0.4339       152\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. FINAL RESULTS"
      ],
      "metadata": {
        "id": "BEI0s8o8zMVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5-FOLD CROSS VALIDATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(fold_results)\n",
        "print(\"\\nResults per fold:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AVERAGE PERFORMANCE ACROSS ALL FOLDS:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:  {results_df['accuracy'].mean():.4f} ± {results_df['accuracy'].std():.4f}\")\n",
        "print(f\"Precision: {results_df['precision'].mean():.4f} ± {results_df['precision'].std():.4f}\")\n",
        "print(f\"Recall:    {results_df['recall'].mean():.4f} ± {results_df['recall'].std():.4f}\")\n",
        "print(f\"F1-Score:  {results_df['f1'].mean():.4f} ± {results_df['f1'].std():.4f}\")\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('bert_lyrics_cv_results.csv', index=False)\n",
        "print(\"\\nResults saved to 'bert_lyrics_cv_results.csv'\")\n",
        "\n",
        "print(\"\\n✅ BERT Lyrics Classification Complete!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Load audio data for audio modalitas\")\n",
        "print(\"2. Load MIDI data for MIDI modalitas\")\n",
        "print(\"3. Implement multimodal fusion\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YaklDkozOwj",
        "outputId": "0515e0fa-e01c-4fd2-b287-eccb0a27a70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "5-FOLD CROSS VALIDATION SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Results per fold:\n",
            " fold  accuracy  precision   recall       f1\n",
            "    1  0.437908   0.440559 0.437908 0.433468\n",
            "    2  0.529412   0.527305 0.529412 0.526101\n",
            "    3  0.490196   0.489881 0.490196 0.472347\n",
            "    4  0.483660   0.515930 0.483660 0.480147\n",
            "    5  0.427632   0.491996 0.427632 0.433879\n",
            "\n",
            "================================================================================\n",
            "AVERAGE PERFORMANCE ACROSS ALL FOLDS:\n",
            "================================================================================\n",
            "Accuracy:  0.4738 ± 0.0415\n",
            "Precision: 0.4931 ± 0.0334\n",
            "Recall:    0.4738 ± 0.0415\n",
            "F1-Score:  0.4692 ± 0.0384\n",
            "\n",
            "Results saved to 'bert_lyrics_cv_results.csv'\n",
            "\n",
            "✅ BERT Lyrics Classification Complete!\n",
            "\n",
            "Next steps:\n",
            "1. Load audio data for audio modalitas\n",
            "2. Load MIDI data for MIDI modalitas\n",
            "3. Implement multimodal fusion\n"
          ]
        }
      ]
    }
  ]
}