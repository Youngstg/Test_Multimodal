{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNODhdqBeAb7il6iyxAYXIN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youngstg/Test_Multimodal/blob/main/TestMultimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTIMODAL MUSIC EMOTION CLASSIFICATION\n",
        "FINAL: Late Fusion of Lyrics (BERT) + Audio (PANNs) + MIDI (Simple Features)\n",
        "\n",
        "Dataset: MIREX Emotion Dataset\n",
        "Strategy: Extract embeddings from each modality ‚Üí Concatenate ‚Üí Classify"
      ],
      "metadata": {
        "id": "A9zhwXWHBZ5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. INSTALLATION & IMPORTS"
      ],
      "metadata": {
        "id": "dXis19CzBcbY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEt8cwrEBZNy",
        "outputId": "02623409-d3d8-4f03-db98-c32900c1ca2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing packages...\n",
            "‚úì Device: cpu\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing packages...\")\n",
        "!pip install -q kagglehub transformers torch panns-inference\n",
        "!pip install -q librosa soundfile pretty_midi\n",
        "!pip install -q scikit-learn pandas numpy\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from panns_inference import AudioTagging\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import librosa\n",
        "import pretty_midi\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'‚úì Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. DOWNLOAD DATASET"
      ],
      "metadata": {
        "id": "ied9Afs0Bf3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DOWNLOADING DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "path = kagglehub.dataset_download(\"imsparsh/multimodal-mirex-emotion-dataset\")\n",
        "print(f\"‚úì Dataset path: {path}\")\n",
        "\n",
        "# Define directories\n",
        "dataset_dir = os.path.join(path, 'dataset')\n",
        "lyrics_dir = os.path.join(dataset_dir, 'Lyrics')\n",
        "audio_dir = os.path.join(dataset_dir, 'Audio')\n",
        "midi_dir = os.path.join(dataset_dir, 'MIDIs')\n",
        "\n",
        "print(f\"‚úì Lyrics: {os.path.exists(lyrics_dir)}\")\n",
        "print(f\"‚úì Audio: {os.path.exists(audio_dir)}\")\n",
        "print(f\"‚úì MIDI: {os.path.exists(midi_dir)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3qWbC4-BhsG",
        "outputId": "c425802b-7b80-4c94-e4be-de09772d2831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "DOWNLOADING DATASET\n",
            "================================================================================\n",
            "‚úì Dataset path: /root/.cache/kagglehub/datasets/imsparsh/multimodal-mirex-emotion-dataset/versions/1\n",
            "‚úì Lyrics: True\n",
            "‚úì Audio: True\n",
            "‚úì MIDI: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. LOAD CLUSTER LABELS"
      ],
      "metadata": {
        "id": "06i43MGnBphj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cluster_labels(dataset_path):\n",
        "    clusters_path = os.path.join(dataset_path, 'dataset', 'clusters.txt')\n",
        "    cluster_labels = []\n",
        "\n",
        "    with open(clusters_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        cluster_labels = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"\\n‚úì Loaded {len(cluster_labels)} cluster labels\")\n",
        "    print(f\"  Unique: {sorted(set(cluster_labels))}\")\n",
        "    return cluster_labels\n",
        "\n",
        "cluster_labels = load_cluster_labels(path)\n",
        "\n",
        "# Create song ID mapping\n",
        "song_cluster_map = {}\n",
        "for idx in range(len(cluster_labels)):\n",
        "    for song_id in [str(idx).zfill(3), str(idx + 1).zfill(3)]:\n",
        "        song_cluster_map[song_id] = cluster_labels[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsnrmzEwBo7U",
        "outputId": "0e9d425b-6c3d-44d0-d98d-1cf4f0dca928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Loaded 903 cluster labels\n",
            "  Unique: ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. LOAD PRE-TRAINED MODELS"
      ],
      "metadata": {
        "id": "V_6A0zA9BuVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING PRE-TRAINED MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# BERT for lyrics\n",
        "print(\"Loading BERT...\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_model.eval()\n",
        "bert_model.to(device)\n",
        "print(\"‚úì BERT loaded\")\n",
        "\n",
        "# PANNs for audio\n",
        "print(\"Loading PANNs...\")\n",
        "panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
        "print(\"‚úì PANNs loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_NeuGcgBu7k",
        "outputId": "84f0b41b-bd05-4ccf-9bbd-7983640a77f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING PRE-TRAINED MODELS\n",
            "================================================================================\n",
            "Loading BERT...\n",
            "‚úì BERT loaded\n",
            "Loading PANNs...\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "Using CPU.\n",
            "‚úì PANNs loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. FEATURE EXTRACTION FUNCTIONS"
      ],
      "metadata": {
        "id": "Oqy6S0m_Bys8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LYRICS FEATURES ---\n",
        "def clean_lyrics(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    text = re.sub(r'[^a-z0-9\\s.,!?\\']', ' ', text)\n",
        "    text = re.sub(r'([.,!?])\\1+', r'\\1', text)\n",
        "    return ' '.join(text.split()).strip()\n",
        "\n",
        "def extract_lyrics_embedding(lyrics, tokenizer, model, max_length=256):\n",
        "    try:\n",
        "        lyrics = clean_lyrics(lyrics)\n",
        "        if not lyrics or len(lyrics) < 10:\n",
        "            return None\n",
        "\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            lyrics,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(device)\n",
        "        attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            embedding = outputs.pooler_output.cpu().numpy()[0]\n",
        "\n",
        "        return embedding\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# --- AUDIO FEATURES ---\n",
        "def extract_audio_embedding(audio_path, panns_model, sr=32000, duration=10):\n",
        "    try:\n",
        "        audio, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
        "\n",
        "        target_length = sr * duration\n",
        "        if len(audio) < target_length:\n",
        "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
        "        else:\n",
        "            audio = audio[:target_length]\n",
        "\n",
        "        _, embedding = panns_model.inference(audio[None, :])\n",
        "        return embedding[0]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# --- MIDI FEATURES ---\n",
        "def extract_midi_features(midi_path):\n",
        "    try:\n",
        "        midi = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "        # Extract statistical features\n",
        "        notes = []\n",
        "        for instrument in midi.instruments:\n",
        "            if not instrument.is_drum:\n",
        "                for note in instrument.notes:\n",
        "                    notes.append({\n",
        "                        'pitch': note.pitch,\n",
        "                        'velocity': note.velocity,\n",
        "                        'duration': note.end - note.start\n",
        "                    })\n",
        "\n",
        "        if len(notes) == 0:\n",
        "            return None\n",
        "\n",
        "        # Compute statistics\n",
        "        pitches = [n['pitch'] for n in notes]\n",
        "        velocities = [n['velocity'] for n in notes]\n",
        "        durations = [n['duration'] for n in notes]\n",
        "\n",
        "        # Tempo\n",
        "        tempo_changes = midi.get_tempo_changes()\n",
        "        avg_tempo = np.mean(tempo_changes[1]) if len(tempo_changes[1]) > 0 else 120.0\n",
        "\n",
        "        # Time signature\n",
        "        time_sigs = midi.time_signature_changes\n",
        "        numerator = time_sigs[0].numerator if len(time_sigs) > 0 else 4\n",
        "        denominator = time_sigs[0].denominator if len(time_sigs) > 0 else 4\n",
        "\n",
        "        # Create feature vector (32-dim)\n",
        "        features = np.array([\n",
        "            # Pitch statistics (8)\n",
        "            np.mean(pitches), np.std(pitches), np.min(pitches), np.max(pitches),\n",
        "            np.percentile(pitches, 25), np.percentile(pitches, 75),\n",
        "            np.ptp(pitches), len(set(pitches)),\n",
        "\n",
        "            # Velocity statistics (8)\n",
        "            np.mean(velocities), np.std(velocities), np.min(velocities), np.max(velocities),\n",
        "            np.percentile(velocities, 25), np.percentile(velocities, 75),\n",
        "            np.ptp(velocities), len(notes),\n",
        "\n",
        "            # Duration statistics (8)\n",
        "            np.mean(durations), np.std(durations), np.min(durations), np.max(durations),\n",
        "            np.percentile(durations, 25), np.percentile(durations, 75),\n",
        "            np.ptp(durations), 1.0 / (np.mean(durations) + 1e-6),\n",
        "\n",
        "            # Temporal features (8)\n",
        "            avg_tempo, avg_tempo / 120.0, numerator, denominator,\n",
        "            numerator / denominator, len(notes) / (midi.get_end_time() + 1e-6),\n",
        "            midi.get_end_time(), len(midi.instruments)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return features\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "IR4YqBBGB35c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. LOAD & EXTRACT ALL FEATURES"
      ],
      "metadata": {
        "id": "KsCoP2yfB5sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXTRACTING FEATURES FROM ALL MODALITIES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "data_list = []\n",
        "\n",
        "# Get all files\n",
        "lyrics_files = {f.replace('.txt', ''): f for f in os.listdir(lyrics_dir) if f.endswith('.txt')}\n",
        "audio_files = {f.replace('.wav', '').replace('.mp3', ''): f for f in os.listdir(audio_dir) if f.endswith(('.wav', '.mp3'))}\n",
        "midi_files = {f.replace('.mid', '').replace('.midi', ''): f for f in os.listdir(midi_dir) if f.endswith(('.mid', '.midi'))}\n",
        "\n",
        "print(f\"Found: {len(lyrics_files)} lyrics, {len(audio_files)} audio, {len(midi_files)} MIDI\")\n",
        "\n",
        "# Get all unique song IDs that have cluster labels\n",
        "all_song_ids = set()\n",
        "for f in lyrics_files.keys():\n",
        "    song_id = ''.join(filter(str.isdigit, f))\n",
        "    if song_id:\n",
        "        all_song_ids.add(song_id.zfill(3))\n",
        "\n",
        "print(f\"\\nProcessing {len(all_song_ids)} songs with multimodal data...\")\n",
        "\n",
        "processed = 0\n",
        "for song_id in sorted(all_song_ids):\n",
        "    if song_id not in song_cluster_map:\n",
        "        continue\n",
        "\n",
        "    # Initialize features\n",
        "    lyrics_emb = None\n",
        "    audio_emb = None\n",
        "    midi_feat = None\n",
        "\n",
        "    # Extract lyrics\n",
        "    for key, filename in lyrics_files.items():\n",
        "        if song_id in key or key.zfill(3) == song_id:\n",
        "            lyrics_path = os.path.join(lyrics_dir, filename)\n",
        "            with open(lyrics_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                lyrics_text = f.read()\n",
        "            lyrics_emb = extract_lyrics_embedding(lyrics_text, bert_tokenizer, bert_model)\n",
        "            break\n",
        "\n",
        "    # Extract audio\n",
        "    for key, filename in audio_files.items():\n",
        "        if song_id in key or key.zfill(3) == song_id:\n",
        "            audio_path = os.path.join(audio_dir, filename)\n",
        "            audio_emb = extract_audio_embedding(audio_path, panns_model)\n",
        "            break\n",
        "\n",
        "    # Extract MIDI\n",
        "    for key, filename in midi_files.items():\n",
        "        if song_id in key or key.zfill(3) == song_id:\n",
        "            midi_path = os.path.join(midi_dir, filename)\n",
        "            midi_feat = extract_midi_features(midi_path)\n",
        "            break\n",
        "\n",
        "    # Only add if at least 2 modalities available\n",
        "    available = sum([lyrics_emb is not None, audio_emb is not None, midi_feat is not None])\n",
        "    if available >= 2:\n",
        "        data_list.append({\n",
        "            'song_id': song_id,\n",
        "            'lyrics_emb': lyrics_emb if lyrics_emb is not None else np.zeros(768),\n",
        "            'audio_emb': audio_emb if audio_emb is not None else np.zeros(2048),\n",
        "            'midi_feat': midi_feat if midi_feat is not None else np.zeros(32),\n",
        "            'has_lyrics': lyrics_emb is not None,\n",
        "            'has_audio': audio_emb is not None,\n",
        "            'has_midi': midi_feat is not None,\n",
        "            'cluster': song_cluster_map[song_id]\n",
        "        })\n",
        "        processed += 1\n",
        "\n",
        "        if processed % 50 == 0:\n",
        "            print(f\"  Processed: {processed} songs...\")\n",
        "\n",
        "print(f\"\\n‚úì Total multimodal samples: {len(data_list)}\")\n",
        "\n",
        "df = pd.DataFrame(data_list)\n",
        "print(f\"‚úì Dataset shape: {df.shape}\")\n",
        "print(f\"\\nModality availability:\")\n",
        "print(f\"  Lyrics: {df['has_lyrics'].sum()} ({df['has_lyrics'].mean()*100:.1f}%)\")\n",
        "print(f\"  Audio: {df['has_audio'].sum()} ({df['has_audio'].mean()*100:.1f}%)\")\n",
        "print(f\"  MIDI: {df['has_midi'].sum()} ({df['has_midi'].mean()*100:.1f}%)\")\n",
        "print(f\"\\nCluster distribution:\")\n",
        "print(df['cluster'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLv2V2OsB7x9",
        "outputId": "d03d552c-a569-4c5b-c621-38d3d0dbc4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "EXTRACTING FEATURES FROM ALL MODALITIES\n",
            "================================================================================\n",
            "Found: 764 lyrics, 903 audio, 196 MIDI\n",
            "\n",
            "Processing 764 songs with multimodal data...\n",
            "  Processed: 50 songs...\n",
            "  Processed: 100 songs...\n",
            "  Processed: 150 songs...\n",
            "  Processed: 200 songs...\n",
            "  Processed: 250 songs...\n",
            "  Processed: 300 songs...\n",
            "  Processed: 350 songs...\n",
            "  Processed: 400 songs...\n",
            "  Processed: 450 songs...\n",
            "  Processed: 500 songs...\n",
            "  Processed: 550 songs...\n",
            "  Processed: 600 songs...\n",
            "  Processed: 650 songs...\n",
            "  Processed: 700 songs...\n",
            "  Processed: 750 songs...\n",
            "\n",
            "‚úì Total multimodal samples: 764\n",
            "‚úì Dataset shape: (764, 8)\n",
            "\n",
            "Modality availability:\n",
            "  Lyrics: 764 (100.0%)\n",
            "  Audio: 764 (100.0%)\n",
            "  MIDI: 191 (25.0%)\n",
            "\n",
            "Cluster distribution:\n",
            "cluster\n",
            "Cluster 3    192\n",
            "Cluster 4    173\n",
            "Cluster 2    138\n",
            "Cluster 1    134\n",
            "Cluster 5    127\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. LABEL ENCODING"
      ],
      "metadata": {
        "id": "h4rB7x28B9Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['cluster'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "print(f\"\\n‚úì Classes: {label_encoder.classes_}\")\n",
        "print(f\"‚úì Number of classes: {num_classes}\")\n",
        "\n",
        "# Class weights\n",
        "y = df['label'].values\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z49W3_lyB-1k",
        "outputId": "47eba8b3-3bc8-4e80-a35d-21fb546b1390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Classes: ['Cluster 1' 'Cluster 2' 'Cluster 3' 'Cluster 4' 'Cluster 5']\n",
            "‚úì Number of classes: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. MULTIMODAL DATASET"
      ],
      "metadata": {
        "id": "Az03jcLUCAkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "\n",
        "        return {\n",
        "            'lyrics_emb': torch.FloatTensor(item['lyrics_emb']),\n",
        "            'audio_emb': torch.FloatTensor(item['audio_emb']),\n",
        "            'midi_feat': torch.FloatTensor(item['midi_feat']),\n",
        "            'has_lyrics': torch.FloatTensor([item['has_lyrics']]),\n",
        "            'has_audio': torch.FloatTensor([item['has_audio']]),\n",
        "            'has_midi': torch.FloatTensor([item['has_midi']]),\n",
        "            'label': torch.tensor(item['label'], dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "mW9945FICCv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. MULTIMODAL FUSION MODEL"
      ],
      "metadata": {
        "id": "yXJi1RcJCGS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalFusionClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Late Fusion: Concatenate embeddings from all modalities\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, lyrics_dim=768, audio_dim=2048, midi_dim=32,\n",
        "                 fusion_dim=512, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Project each modality to common dimension\n",
        "        self.lyrics_proj = nn.Sequential(\n",
        "            nn.Linear(lyrics_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout * 0.5)\n",
        "        )\n",
        "\n",
        "        self.audio_proj = nn.Sequential(\n",
        "            nn.Linear(audio_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout * 0.5)\n",
        "        )\n",
        "\n",
        "        self.midi_proj = nn.Sequential(\n",
        "            nn.Linear(midi_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout * 0.5)\n",
        "        )\n",
        "\n",
        "        # Fusion layer\n",
        "        fusion_input_dim = 256 + 256 + 128  # 640\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(fusion_input_dim, fusion_dim),\n",
        "            nn.BatchNorm1d(fusion_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout * 0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, lyrics_emb, audio_emb, midi_feat, has_lyrics, has_audio, has_midi):\n",
        "        # Project each modality\n",
        "        lyrics_feat = self.lyrics_proj(lyrics_emb)\n",
        "        audio_feat = self.audio_proj(audio_emb)\n",
        "        midi_feat_proj = self.midi_proj(midi_feat)\n",
        "\n",
        "        # Mask unavailable modalities\n",
        "        lyrics_feat = lyrics_feat * has_lyrics\n",
        "        audio_feat = audio_feat * has_audio\n",
        "        midi_feat_proj = midi_feat_proj * has_midi\n",
        "\n",
        "        # Concatenate\n",
        "        fused = torch.cat([lyrics_feat, audio_feat, midi_feat_proj], dim=1)\n",
        "\n",
        "        # Classify\n",
        "        logits = self.fusion(fused)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Rx0e29aRCIbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. TRAINING & EVALUATION"
      ],
      "metadata": {
        "id": "GIyUvjVXCM1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        lyrics = batch['lyrics_emb'].to(device)\n",
        "        audio = batch['audio_emb'].to(device)\n",
        "        midi = batch['midi_feat'].to(device)\n",
        "        has_l = batch['has_lyrics'].to(device)\n",
        "        has_a = batch['has_audio'].to(device)\n",
        "        has_m = batch['has_midi'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(lyrics, audio, midi, has_l, has_a, has_m)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return total_loss / len(dataloader), accuracy_score(true_labels, predictions)\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            lyrics = batch['lyrics_emb'].to(device)\n",
        "            audio = batch['audio_emb'].to(device)\n",
        "            midi = batch['midi_feat'].to(device)\n",
        "            has_l = batch['has_lyrics'].to(device)\n",
        "            has_a = batch['has_audio'].to(device)\n",
        "            has_m = batch['has_midi'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(lyrics, audio, midi, has_l, has_a, has_m)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(true_labels, predictions)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', zero_division=0)\n",
        "    return total_loss / len(dataloader), acc, p, r, f1, predictions, true_labels"
      ],
      "metadata": {
        "id": "uutm5ROgCPAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. 5-FOLD CROSS VALIDATION"
      ],
      "metadata": {
        "id": "M57RWVESCQwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "LR = 5e-5\n",
        "EPOCHS = 25\n",
        "PATIENCE = 7\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MULTIMODAL FUSION - 5-FOLD CROSS VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Modalities: Lyrics (768) + Audio (2048) + MIDI (32)\")\n",
        "print(f\"Fusion strategy: Late concatenation ‚Üí MLP\")\n",
        "\n",
        "X = df.index.values\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FOLD {fold + 1}/5\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    train_data = df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_data = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = MultimodalDataset(train_data)\n",
        "    val_dataset = MultimodalDataset(val_data)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    model = MultimodalFusionClassifier(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=3)\n",
        "\n",
        "    best_f1 = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc, val_p, val_r, val_f1, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step(val_f1)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}, F1={val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            torch.save(model.state_dict(), f'best_multimodal_fold{fold+1}.pt')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(f'best_multimodal_fold{fold+1}.pt'))\n",
        "    val_loss, val_acc, val_p, val_r, val_f1, preds, labels = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"\\nFold {fold+1} Results: Acc={val_acc:.4f}, Precision={val_p:.4f}, Recall={val_r:.4f}, F1={val_f1:.4f}\")\n",
        "    print(classification_report(labels, preds, target_names=label_encoder.classes_, digits=4, zero_division=0))\n",
        "\n",
        "    fold_results.append({'fold': fold+1, 'accuracy': val_acc, 'precision': val_p, 'recall': val_r, 'f1': val_f1})\n",
        "\n",
        "# ============================================================================\n",
        "# 12. FINAL RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MULTIMODAL RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(fold_results)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nAverage Performance:\")\n",
        "print(f\"  Accuracy:  {results_df['accuracy'].mean():.4f} ¬± {results_df['accuracy'].std():.4f}\")\n",
        "print(f\"  Precision: {results_df['precision'].mean():.4f} ¬± {results_df['precision'].std():.4f}\")\n",
        "print(f\"  Recall:    {results_df['recall'].mean():.4f} ¬± {results_df['recall'].std():.4f}\")\n",
        "print(f\"  F1-Score:  {results_df['f1'].mean():.4f} ¬± {results_df['f1'].std():.4f}\")\n",
        "\n",
        "results_df.to_csv('multimodal_fusion_results.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON WITH SINGLE MODALITIES\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Lyrics (BERT) only:     ~45-55% F1\")\n",
        "print(f\"Audio (PANNs) only:     ~50-60% F1\")\n",
        "print(f\"MIDI (Orpheus) only:    ~23% F1\")\n",
        "print(f\"MULTIMODAL FUSION:      ~{results_df['f1'].mean():.1%} F1\")\n",
        "\n",
        "if results_df['f1'].mean() > 0.60:\n",
        "    print(\"\\nüéâ SUCCESS! Multimodal fusion outperforms single modalities!\")\n",
        "elif results_df['f1'].mean() > 0.55:\n",
        "    print(\"\\n‚úì Good! Multimodal provides improvement.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Multimodal similar to best single modality (Audio).\")\n",
        "\n",
        "print(\"\\n‚úÖ COMPLETE!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL5hLPctCVJ5",
        "outputId": "58da4b2e-e744-49ef-b189-9114d88a4204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MULTIMODAL FUSION - 5-FOLD CROSS VALIDATION\n",
            "================================================================================\n",
            "Total samples: 764\n",
            "Modalities: Lyrics (768) + Audio (2048) + MIDI (32)\n",
            "Fusion strategy: Late concatenation ‚Üí MLP\n",
            "\n",
            "================================================================================\n",
            "FOLD 1/5\n",
            "================================================================================\n",
            "Epoch 1/25: Train Acc=0.2422, Val Acc=0.3529, F1=0.2935\n",
            "Epoch 2/25: Train Acc=0.3453, Val Acc=0.4641, F1=0.4264\n",
            "Epoch 3/25: Train Acc=0.3732, Val Acc=0.5098, F1=0.4901\n",
            "Epoch 4/25: Train Acc=0.4501, Val Acc=0.5033, F1=0.4710\n",
            "Epoch 5/25: Train Acc=0.4092, Val Acc=0.4706, F1=0.4280\n",
            "Epoch 6/25: Train Acc=0.4534, Val Acc=0.4837, F1=0.4381\n",
            "Epoch 7/25: Train Acc=0.4632, Val Acc=0.4902, F1=0.4523\n",
            "Epoch 8/25: Train Acc=0.4664, Val Acc=0.4967, F1=0.4406\n",
            "Epoch 9/25: Train Acc=0.4746, Val Acc=0.4837, F1=0.4347\n",
            "Epoch 10/25: Train Acc=0.4992, Val Acc=0.4510, F1=0.4096\n",
            "Early stopping!\n",
            "\n",
            "Fold 1 Results: Acc=0.5098, Precision=0.4888, Recall=0.5098, F1=0.4901\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.2381    0.1852    0.2083        27\n",
            "   Cluster 2     0.4231    0.3929    0.4074        28\n",
            "   Cluster 3     0.6889    0.8158    0.7470        38\n",
            "   Cluster 4     0.5000    0.3429    0.4068        35\n",
            "   Cluster 5     0.5135    0.7600    0.6129        25\n",
            "\n",
            "    accuracy                         0.5098       153\n",
            "   macro avg     0.4727    0.4993    0.4765       153\n",
            "weighted avg     0.4888    0.5098    0.4901       153\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 2/5\n",
            "================================================================================\n",
            "Epoch 1/25: Train Acc=0.2095, Val Acc=0.3725, F1=0.3047\n",
            "Epoch 2/25: Train Acc=0.3077, Val Acc=0.3856, F1=0.3327\n",
            "Epoch 3/25: Train Acc=0.3568, Val Acc=0.4510, F1=0.4063\n",
            "Epoch 4/25: Train Acc=0.4108, Val Acc=0.4641, F1=0.4178\n",
            "Epoch 5/25: Train Acc=0.4354, Val Acc=0.4444, F1=0.4069\n",
            "Epoch 6/25: Train Acc=0.4566, Val Acc=0.4575, F1=0.4154\n",
            "Epoch 7/25: Train Acc=0.4861, Val Acc=0.4641, F1=0.4296\n",
            "Epoch 8/25: Train Acc=0.4975, Val Acc=0.4510, F1=0.4208\n",
            "Epoch 9/25: Train Acc=0.4845, Val Acc=0.4248, F1=0.4020\n",
            "Epoch 10/25: Train Acc=0.5254, Val Acc=0.4575, F1=0.4350\n",
            "Epoch 11/25: Train Acc=0.5090, Val Acc=0.4641, F1=0.4360\n",
            "Epoch 12/25: Train Acc=0.5417, Val Acc=0.4575, F1=0.4405\n",
            "Epoch 13/25: Train Acc=0.5303, Val Acc=0.4575, F1=0.4319\n",
            "Epoch 14/25: Train Acc=0.5483, Val Acc=0.4641, F1=0.4533\n",
            "Epoch 15/25: Train Acc=0.5859, Val Acc=0.4575, F1=0.4204\n",
            "Epoch 16/25: Train Acc=0.5810, Val Acc=0.4444, F1=0.4403\n",
            "Epoch 17/25: Train Acc=0.5777, Val Acc=0.4575, F1=0.4412\n",
            "Epoch 18/25: Train Acc=0.6137, Val Acc=0.4706, F1=0.4609\n",
            "Epoch 19/25: Train Acc=0.6268, Val Acc=0.4510, F1=0.4388\n",
            "Epoch 20/25: Train Acc=0.6432, Val Acc=0.4510, F1=0.4472\n",
            "Epoch 21/25: Train Acc=0.6547, Val Acc=0.4575, F1=0.4548\n",
            "Epoch 22/25: Train Acc=0.6858, Val Acc=0.4902, F1=0.4838\n",
            "Epoch 23/25: Train Acc=0.6972, Val Acc=0.4967, F1=0.4920\n",
            "Epoch 24/25: Train Acc=0.6710, Val Acc=0.4706, F1=0.4610\n",
            "Epoch 25/25: Train Acc=0.7169, Val Acc=0.4967, F1=0.4825\n",
            "\n",
            "Fold 2 Results: Acc=0.4967, Precision=0.4920, Recall=0.4967, F1=0.4920\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.3500    0.2593    0.2979        27\n",
            "   Cluster 2     0.4545    0.5357    0.4918        28\n",
            "   Cluster 3     0.5833    0.5526    0.5676        38\n",
            "   Cluster 4     0.5143    0.5143    0.5143        35\n",
            "   Cluster 5     0.5172    0.6000    0.5556        25\n",
            "\n",
            "    accuracy                         0.4967       153\n",
            "   macro avg     0.4839    0.4924    0.4854       153\n",
            "weighted avg     0.4920    0.4967    0.4920       153\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 3/5\n",
            "================================================================================\n",
            "Epoch 1/25: Train Acc=0.2095, Val Acc=0.2614, F1=0.2210\n",
            "Epoch 2/25: Train Acc=0.3273, Val Acc=0.3791, F1=0.3297\n",
            "Epoch 3/25: Train Acc=0.3519, Val Acc=0.4052, F1=0.3719\n",
            "Epoch 4/25: Train Acc=0.4272, Val Acc=0.3725, F1=0.3349\n",
            "Epoch 5/25: Train Acc=0.4255, Val Acc=0.3725, F1=0.3177\n",
            "Epoch 6/25: Train Acc=0.4337, Val Acc=0.4118, F1=0.3767\n",
            "Epoch 7/25: Train Acc=0.4763, Val Acc=0.4379, F1=0.4112\n",
            "Epoch 8/25: Train Acc=0.5057, Val Acc=0.4314, F1=0.4015\n",
            "Epoch 9/25: Train Acc=0.4877, Val Acc=0.4118, F1=0.3799\n",
            "Epoch 10/25: Train Acc=0.5188, Val Acc=0.4248, F1=0.4041\n",
            "Epoch 11/25: Train Acc=0.5155, Val Acc=0.4248, F1=0.4113\n",
            "Epoch 12/25: Train Acc=0.5614, Val Acc=0.4052, F1=0.3873\n",
            "Epoch 13/25: Train Acc=0.5679, Val Acc=0.4052, F1=0.4040\n",
            "Epoch 14/25: Train Acc=0.5777, Val Acc=0.3987, F1=0.3888\n",
            "Epoch 15/25: Train Acc=0.5843, Val Acc=0.4248, F1=0.4279\n",
            "Epoch 16/25: Train Acc=0.5876, Val Acc=0.4248, F1=0.4113\n",
            "Epoch 17/25: Train Acc=0.6023, Val Acc=0.4248, F1=0.4282\n",
            "Epoch 18/25: Train Acc=0.6219, Val Acc=0.4510, F1=0.4442\n",
            "Epoch 19/25: Train Acc=0.6334, Val Acc=0.4183, F1=0.4103\n",
            "Epoch 20/25: Train Acc=0.6596, Val Acc=0.4248, F1=0.4201\n",
            "Epoch 21/25: Train Acc=0.6645, Val Acc=0.4510, F1=0.4437\n",
            "Epoch 22/25: Train Acc=0.6972, Val Acc=0.4444, F1=0.4309\n",
            "Epoch 23/25: Train Acc=0.6907, Val Acc=0.4510, F1=0.4474\n",
            "Epoch 24/25: Train Acc=0.6858, Val Acc=0.4444, F1=0.4388\n",
            "Epoch 25/25: Train Acc=0.7218, Val Acc=0.4248, F1=0.4208\n",
            "\n",
            "Fold 3 Results: Acc=0.4510, Precision=0.4481, Recall=0.4510, F1=0.4474\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.3333    0.2593    0.2917        27\n",
            "   Cluster 2     0.3125    0.3704    0.3390        27\n",
            "   Cluster 3     0.6279    0.6923    0.6585        39\n",
            "   Cluster 4     0.5000    0.4412    0.4688        34\n",
            "   Cluster 5     0.3704    0.3846    0.3774        26\n",
            "\n",
            "    accuracy                         0.4510       153\n",
            "   macro avg     0.4288    0.4295    0.4271       153\n",
            "weighted avg     0.4481    0.4510    0.4474       153\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 4/5\n",
            "================================================================================\n",
            "Epoch 1/25: Train Acc=0.2144, Val Acc=0.2876, F1=0.2192\n",
            "Epoch 2/25: Train Acc=0.3011, Val Acc=0.4379, F1=0.4058\n",
            "Epoch 3/25: Train Acc=0.3339, Val Acc=0.4771, F1=0.4467\n",
            "Epoch 4/25: Train Acc=0.3879, Val Acc=0.4575, F1=0.4263\n",
            "Epoch 5/25: Train Acc=0.4206, Val Acc=0.4967, F1=0.4591\n",
            "Epoch 6/25: Train Acc=0.4484, Val Acc=0.5294, F1=0.5006\n",
            "Epoch 7/25: Train Acc=0.4583, Val Acc=0.5229, F1=0.4954\n",
            "Epoch 8/25: Train Acc=0.4403, Val Acc=0.4967, F1=0.4586\n",
            "Epoch 9/25: Train Acc=0.4763, Val Acc=0.5294, F1=0.5059\n",
            "Epoch 10/25: Train Acc=0.4779, Val Acc=0.4967, F1=0.4607\n",
            "Epoch 11/25: Train Acc=0.5074, Val Acc=0.4837, F1=0.4499\n",
            "Epoch 12/25: Train Acc=0.4812, Val Acc=0.4837, F1=0.4566\n",
            "Epoch 13/25: Train Acc=0.5155, Val Acc=0.5033, F1=0.4823\n",
            "Epoch 14/25: Train Acc=0.5581, Val Acc=0.5033, F1=0.4836\n",
            "Epoch 15/25: Train Acc=0.5712, Val Acc=0.5098, F1=0.4934\n",
            "Epoch 16/25: Train Acc=0.5827, Val Acc=0.5229, F1=0.5059\n",
            "Epoch 17/25: Train Acc=0.5810, Val Acc=0.5229, F1=0.4913\n",
            "Epoch 18/25: Train Acc=0.5712, Val Acc=0.5359, F1=0.5132\n",
            "Epoch 19/25: Train Acc=0.6105, Val Acc=0.5359, F1=0.5229\n",
            "Epoch 20/25: Train Acc=0.6056, Val Acc=0.5229, F1=0.5045\n",
            "Epoch 21/25: Train Acc=0.5892, Val Acc=0.5425, F1=0.5313\n",
            "Epoch 22/25: Train Acc=0.5990, Val Acc=0.5359, F1=0.5207\n",
            "Epoch 23/25: Train Acc=0.5777, Val Acc=0.5425, F1=0.5242\n",
            "Epoch 24/25: Train Acc=0.5974, Val Acc=0.5359, F1=0.5193\n",
            "Epoch 25/25: Train Acc=0.6154, Val Acc=0.5359, F1=0.5208\n",
            "\n",
            "Fold 4 Results: Acc=0.5425, Precision=0.5317, Recall=0.5425, F1=0.5313\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.3810    0.2963    0.3333        27\n",
            "   Cluster 2     0.5312    0.6296    0.5763        27\n",
            "   Cluster 3     0.6829    0.7179    0.7000        39\n",
            "   Cluster 4     0.4800    0.3529    0.4068        34\n",
            "   Cluster 5     0.5294    0.6923    0.6000        26\n",
            "\n",
            "    accuracy                         0.5425       153\n",
            "   macro avg     0.5209    0.5378    0.5233       153\n",
            "weighted avg     0.5317    0.5425    0.5313       153\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FOLD 5/5\n",
            "================================================================================\n",
            "Epoch 1/25: Train Acc=0.2663, Val Acc=0.2632, F1=0.2413\n",
            "Epoch 2/25: Train Acc=0.3448, Val Acc=0.3553, F1=0.3174\n",
            "Epoch 3/25: Train Acc=0.3856, Val Acc=0.3750, F1=0.3585\n",
            "Epoch 4/25: Train Acc=0.4510, Val Acc=0.4013, F1=0.3789\n",
            "Epoch 5/25: Train Acc=0.4363, Val Acc=0.4276, F1=0.4036\n",
            "Epoch 6/25: Train Acc=0.4690, Val Acc=0.4276, F1=0.4032\n",
            "Epoch 7/25: Train Acc=0.4739, Val Acc=0.4013, F1=0.3852\n",
            "Epoch 8/25: Train Acc=0.4967, Val Acc=0.4145, F1=0.3869\n",
            "Epoch 9/25: Train Acc=0.5033, Val Acc=0.3882, F1=0.3697\n",
            "Epoch 10/25: Train Acc=0.5278, Val Acc=0.3750, F1=0.3628\n",
            "Epoch 11/25: Train Acc=0.5359, Val Acc=0.3816, F1=0.3683\n",
            "Epoch 12/25: Train Acc=0.5392, Val Acc=0.3750, F1=0.3646\n",
            "Early stopping!\n",
            "\n",
            "Fold 5 Results: Acc=0.4276, Precision=0.4234, Recall=0.4276, F1=0.4036\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cluster 1     0.4286    0.2308    0.3000        26\n",
            "   Cluster 2     0.4737    0.3214    0.3830        28\n",
            "   Cluster 3     0.5455    0.7895    0.6452        38\n",
            "   Cluster 4     0.3333    0.2000    0.2500        35\n",
            "   Cluster 5     0.3023    0.5200    0.3824        25\n",
            "\n",
            "    accuracy                         0.4276       152\n",
            "   macro avg     0.4167    0.4123    0.3921       152\n",
            "weighted avg     0.4234    0.4276    0.4036       152\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FINAL MULTIMODAL RESULTS\n",
            "================================================================================\n",
            " fold  accuracy  precision   recall       f1\n",
            "    1  0.509804   0.488826 0.509804 0.490051\n",
            "    2  0.496732   0.491993 0.496732 0.491958\n",
            "    3  0.450980   0.448075 0.450980 0.447446\n",
            "    4  0.542484   0.531688 0.542484 0.531306\n",
            "    5  0.427632   0.423409 0.427632 0.403608\n",
            "\n",
            "Average Performance:\n",
            "  Accuracy:  0.4855 ¬± 0.0461\n",
            "  Precision: 0.4768 ¬± 0.0420\n",
            "  Recall:    0.4855 ¬± 0.0461\n",
            "  F1-Score:  0.4729 ¬± 0.0488\n",
            "\n",
            "================================================================================\n",
            "COMPARISON WITH SINGLE MODALITIES\n",
            "================================================================================\n",
            "Lyrics (BERT) only:     ~45-55% F1\n",
            "Audio (PANNs) only:     ~50-60% F1\n",
            "MIDI (Orpheus) only:    ~23% F1\n",
            "MULTIMODAL FUSION:      ~47.3% F1\n",
            "\n",
            "‚ö†Ô∏è Multimodal similar to best single modality (Audio).\n",
            "\n",
            "‚úÖ COMPLETE!\n"
          ]
        }
      ]
    }
  ]
}